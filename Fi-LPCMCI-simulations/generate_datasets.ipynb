{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "5 nodes "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "         y1        y2        y3        y4        y5\n",
      "0  0.973723 -0.277603  1.523919  0.528692  0.116470\n",
      "1  0.734256 -1.618485  1.927383  1.655883  0.853793\n",
      "2  0.590121 -1.672467 -0.657566  3.388754  4.274545\n",
      "3 -1.728108 -0.634837 -0.573889 -0.500856  1.259387\n",
      "4 -0.708397  0.229302 -1.030391  1.078399  3.217724\n"
     ]
    }
   ],
   "source": [
    "# Function to generate a dataset and its ground truth causal matrix\n",
    "def generate_dataset(n, nan_ratio=0.1, phi1=0.8, phi2=0.7):\n",
    "\n",
    "    # Initialize latent confounders\n",
    "    latent1 = np.zeros(n)\n",
    "    latent2 = np.zeros(n)\n",
    "    latent1[0] = np.random.normal()\n",
    "    latent2[0] = np.random.normal()\n",
    "\n",
    "    # Generate latent confounders using AR(1) processes\n",
    "    for t in range(1, n):\n",
    "        latent1[t] = phi1 * latent1[t-1] + np.random.normal()\n",
    "        latent2[t] = phi2 * latent2[t-1] + np.random.normal()\n",
    "\n",
    "    # Initialize system variables y1 to y5\n",
    "    y1 = np.zeros(n)\n",
    "    y2 = np.zeros(n)\n",
    "    y3 = np.zeros(n)\n",
    "    y4 = np.zeros(n)\n",
    "    y5 = np.zeros(n)\n",
    "\n",
    "    # Generate y1 influenced by y2 (one time lag) and latent1\n",
    "    y1[0] = np.random.normal()\n",
    "    for t in range(1, n):\n",
    "        y1[t] = 0.5 * y2[t-1] + 0.6 * latent1[t] + np.random.normal()\n",
    "\n",
    "    # Generate y2 using AR(1) process (y2 causes itself over time)\n",
    "    y2[0] = np.random.normal()\n",
    "    for t in range(1, n):\n",
    "        y2[t] = 0.7 * y2[t-1] + 0.8 * latent2[t] + np.random.normal()\n",
    "\n",
    "    # Generate y3 influenced by latent2\n",
    "    y3[0] = np.random.normal()\n",
    "    for t in range(1, n):\n",
    "        y3[t] = 0.8 * latent2[t] + np.random.normal()\n",
    "\n",
    "    # Generate y4 influenced by y3 (one time lag) and latent1\n",
    "    y4[0] = np.random.normal()\n",
    "    for t in range(1, n):\n",
    "        y4[t] = 0.8 * y3[t-1] + 0.5 * latent1[t] + np.random.normal()\n",
    "\n",
    "    # Generate y5 influenced by y4 (contemporaneous), self-lag, and latent2\n",
    "    y5[0] = np.random.normal()\n",
    "    for t in range(1, n):\n",
    "        y5[t] = 0.6 * y5[t-1] + 0.9 * y4[t] + np.random.normal()\n",
    "\n",
    "    # Introduce NaN values randomly\n",
    "    nan_indices = np.random.choice(n, size=int(n * nan_ratio), replace=False)\n",
    "    for y in [y1, y2, y3, y4, y5]:\n",
    "        y[nan_indices] = np.nan\n",
    "\n",
    "    # Combine into DataFrame\n",
    "    data = pd.DataFrame({'y1': y1, 'y2': y2, 'y3': y3, 'y4': y4, 'y5': y5})\n",
    "\n",
    "    # Ground truth causal matrix\n",
    "    ground_truth = np.array([\n",
    "        # y1 caused by y2[t-1] and latent1\n",
    "        [['', ''],  # y1 not influencing anything\n",
    "         ['', ''],  # y1 caused by y2[t-1]\n",
    "         ['', ''],     # No relation with y3\n",
    "         ['o-o', ''],  # y1 caused by latent1\n",
    "         ['', '']],    # No relation with y5\n",
    "\n",
    "        # y2 causes itself over time (AR(1)) and causes y1[t-1]\n",
    "        [['', '-->'],    # y2 not caused by y1\n",
    "         ['', '-->'],  # y2 causes itself at lag 1\n",
    "         ['o-o', ''],  # y2 caused by latent2\n",
    "         ['', ''],    # No relation with y4\n",
    "         ['', '']],   # No relation with y5\n",
    "\n",
    "        # y3 caused by latent2, causes y4[t-1]\n",
    "        [['', ''],    # No relation with y1\n",
    "         ['o-o', ''], # y3 caused by latent2\n",
    "         ['', ''],    # y3 not influenced by itself\n",
    "         ['', '-->'], # y3[t-1] causes y4\n",
    "         ['', '']],   # No relation with y5\n",
    "\n",
    "        # y4 caused by y3[t-1] and latent1, causes y5 contemporaneously\n",
    "        [['o-o', ''],    # No relation with y1\n",
    "         ['', ''],    # No relation with y2\n",
    "         ['', ''], # y4 caused by y3[t-1]\n",
    "         ['-->', ''],    # y4 not influenced by itself\n",
    "         ['-->', '']], # y4 causes y5 contemporaneously\n",
    "\n",
    "        # y5 caused by y4 contemporaneously and self at lag\n",
    "        [['', ''],    # No relation with y1\n",
    "         ['', ''],    # No relation with y2\n",
    "         ['o-o', ''], # y5 caused by latent2\n",
    "         ['<--', ''], # y5 caused by y4 contemporaneously\n",
    "         ['', '-->']]  # y5 causes itself at lag\n",
    "    ])\n",
    "\n",
    "    return data, ground_truth\n",
    "\n",
    "# Example usage:\n",
    "n = 300  # Length of the time series\n",
    "nan_ratio = 0.1  # Ratio of NaN values\n",
    "\n",
    "# Generate dataset and ground truth\n",
    "data, ground_truth = generate_dataset(n, nan_ratio)\n",
    "\n",
    "# Show the generated dataset and ground truth\n",
    "print(data.head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# Function to generate the dataset and its ground truth causal matrix\n",
    "def generate_causal_dataset(n, nan_ratio=0.1, phi_latent=0.8):\n",
    "    # Initialize latent confounder\n",
    "    latent1 = np.zeros(n)\n",
    "    latent1[0] = np.random.normal()\n",
    "\n",
    "    # Generate latent confounder using an AR(1) process\n",
    "    for t in range(1, n):\n",
    "        latent1[t] = phi_latent * latent1[t-1] + np.random.normal()\n",
    "\n",
    "    # Initialize variables y1 to y5\n",
    "    y1 = np.zeros(n)\n",
    "    y2 = np.zeros(n)\n",
    "    y3 = np.zeros(n)\n",
    "    y4 = np.zeros(n)\n",
    "    y5 = np.zeros(n)\n",
    "\n",
    "    # Generate y2 influenced by latent1\n",
    "    y2[0] = np.random.normal()\n",
    "    for t in range(1, n):\n",
    "        y2[t] = 0.6 * latent1[t] + np.random.normal()\n",
    "\n",
    "    # Generate y1 influenced by y2[t] (contemporaneous)\n",
    "    y1[0] = np.random.normal()\n",
    "    for t in range(1, n):\n",
    "        y1[t] = 0.5 * y2[t] + np.random.normal()\n",
    "\n",
    "    # Generate y3 influenced by y1[t-1]\n",
    "    y3[0] = np.random.normal()\n",
    "    for t in range(1, n):\n",
    "        y3[t] = 0.7 * y1[t-1] + np.random.normal()\n",
    "\n",
    "    # Generate y4 influenced by y2[t-1] and y3[t] (contemporaneous)\n",
    "    y4[0] = np.random.normal()\n",
    "    for t in range(1, n):\n",
    "        y4[t] = 0.5 * y2[t-1] + 0.8 * y3[t] + np.random.normal()\n",
    "\n",
    "    # Generate y5 influenced by latent1 and self (y5[t-1])\n",
    "    y5[0] = np.random.normal()\n",
    "    for t in range(1, n):\n",
    "        y5[t] = 0.6 * latent1[t] + 0.5 * y5[t-1] + np.random.normal()\n",
    "\n",
    "    # Introduce NaN values randomly\n",
    "    nan_indices = np.random.choice(n, size=int(n * nan_ratio), replace=False)\n",
    "    for y in [y1, y2, y3, y4, y5]:\n",
    "        y[nan_indices] = np.nan\n",
    "\n",
    "    # Combine into DataFrame\n",
    "    data = pd.DataFrame({'y1': y1, 'y2': y2, 'y3': y3, 'y4': y4, 'y5': y5})\n",
    "\n",
    "    # Ground truth causal matrix\n",
    "        # Ground truth causal matrix\n",
    "    ground_truth = np.array([\n",
    "        [['', ''], ['<--', ''], ['', '-->'], ['', ''], ['', '']],\n",
    "        [['-->', ''], ['', ''], ['', ''], ['', '-->'], ['o-o', '']],\n",
    "        [['', ''], ['', ''], ['', ''], ['-->', ''], ['', '']],\n",
    "        [['', ''], ['', ''], ['<--', ''], ['', ''], ['', '']],\n",
    "        [['', ''], ['o-o', ''], ['', ''], ['', ''], ['', '-->']]\n",
    "    ])\n",
    "\n",
    "    return data, ground_truth\n",
    "\n",
    "def generate_dataset(n, nan_ratio, phi=0.8):\n",
    "    \n",
    "    # Initialize latent confounder\n",
    "    latent = np.zeros(n)\n",
    "    latent[0] = np.random.normal()\n",
    "    for t in range(1, n):\n",
    "        latent[t] = phi * latent[t-1] + np.random.normal()\n",
    "\n",
    "    # Initialize variables y1 to y5\n",
    "    y1 = np.zeros(n)\n",
    "    y2 = np.zeros(n)\n",
    "    y3 = np.zeros(n)\n",
    "    y4 = np.zeros(n)\n",
    "    y5 = np.zeros(n)\n",
    "\n",
    "    # Generate y1 using AR(1) with non-linear transformation\n",
    "    y1[0] = np.random.normal()\n",
    "    for t in range(1, n):\n",
    "        y1[t] = 0.9 * y1[t-1] + np.random.normal()\n",
    "    y1 = np.log(np.abs(y1) + 1) + np.random.normal(scale=0.1, size=n)\n",
    "\n",
    "    # Generate y2 influenced by latent and y1\n",
    "    y2[0] = np.random.normal()\n",
    "    for t in range(1, n):\n",
    "        y2[t] = 0.8 * latent[t] + 0.3 * np.tanh(y1[t]) + np.random.normal()\n",
    "\n",
    "    # Generate y3 influenced by latent and y2\n",
    "    y3[0] = np.random.normal()\n",
    "    for t in range(1, n):\n",
    "        y3[t] = 0.6 * latent[t] + np.random.normal()\n",
    "\n",
    "    # Generate y4 influenced by lagged y1 and y3\n",
    "    y4[0] = np.random.normal()\n",
    "    for t in range(1, n):\n",
    "        y4[t] = 0.4 * np.roll(y1, 1)[t] + 0.9 * np.sqrt(np.abs(y3[t])) + np.random.normal()\n",
    "\n",
    "    # Generate y5 influenced by latent, y2, and lagged y4\n",
    "    y5[0] = np.random.normal()\n",
    "    for t in range(1, n):\n",
    "        y5[t] = 0.7 * latent[t] + 0.5 * np.log(np.abs(y2[t]) + 1) + 0.6 * np.roll(y4, 1)[t] + np.random.normal()\n",
    "\n",
    "    # Introduce NaN values randomly\n",
    "    nan_indices = np.random.choice(n, size=int(n * nan_ratio), replace=False)\n",
    "    for y in [y1, y2, y3, y4, y5]:\n",
    "        y[nan_indices] = np.nan\n",
    "\n",
    "    # Combine into DataFrame\n",
    "    data = pd.DataFrame({'y0': y1, 'y1': y2, 'y2': y3, 'y3': y4, 'y4': y5})\n",
    "\n",
    "    # Ground truth causal matrix\n",
    "    ground_truth = np.array([\n",
    "        [['', ''], ['<--', ''], ['', '-->'], ['', ''], ['', '']],\n",
    "        [['-->', ''], ['', ''], ['', ''], ['', '-->'], ['o-o', '']],\n",
    "        [['', ''], ['', ''], ['', ''], ['-->', ''], ['', '']],\n",
    "        [['', ''], ['', ''], ['<--', ''], ['', ''], ['', '']],\n",
    "        [['', ''], ['o-o', ''], ['', ''], ['', ''], ['', '-->']]\n",
    "    ])\n",
    "\n",
    "    return data, ground_truth"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to generate the dataset and its ground truth causal matrix\n",
    "def generate_causal_dataset(n, nan_ratio=0.1, phi_latent1=0.8, phi_latent2=0.7):\n",
    "    # Initialize latent confounders\n",
    "    latent1 = np.zeros(n)\n",
    "    latent2 = np.zeros(n)\n",
    "    latent1[0] = np.random.laplace()  # Non-Gaussian distribution (Laplace)\n",
    "    latent2[0] = np.random.laplace()  # Non-Gaussian distribution (Laplace)\n",
    "\n",
    "    # Generate latent confounders using AR(1) processes with Laplace distribution\n",
    "    for t in range(1, n):\n",
    "        latent1[t] = phi_latent1 * latent1[t-1] + np.random.laplace()\n",
    "        latent2[t] = phi_latent2 * latent2[t-1] + np.random.laplace()\n",
    "\n",
    "    # Initialize variables y1 to y5\n",
    "    y1 = np.zeros(n)\n",
    "    y2 = np.zeros(n)\n",
    "    y3 = np.zeros(n)\n",
    "    y4 = np.zeros(n)\n",
    "    y5 = np.zeros(n)\n",
    "\n",
    "    # Generate y1 influenced by latent1 and itself (Non-linear and Non-Gaussian)\n",
    "    y1[0] = np.random.laplace()\n",
    "    for t in range(1, n):\n",
    "        y1[t] = np.tanh(0.6 * y1[t-1]) + 0.4 * np.sin(latent1[t]) + np.random.laplace()\n",
    "\n",
    "    # Generate y2 influenced by latent1 and itself (Non-linear and Non-Gaussian)\n",
    "    y2[0] = np.random.laplace()\n",
    "    for t in range(1, n):\n",
    "        y2[t] = np.tanh(0.7 * y2[t-1]) + 0.5 * np.exp(latent1[t]) + np.random.laplace()\n",
    "\n",
    "    # Generate y3 influenced by y2[t] and itself (Non-linear and Non-Gaussian)\n",
    "    y3[0] = np.random.laplace()\n",
    "    for t in range(1, n):\n",
    "        y3[t] = np.tanh(0.8 * y3[t-1]) + 0.6 * np.sin(y2[t]) + 0.5 * np.tanh(latent2[t]) + np.random.laplace()\n",
    "\n",
    "    # Generate y4 influenced by y3[t-1] (Non-linear and Non-Gaussian)\n",
    "    y4[0] = np.random.laplace()\n",
    "    for t in range(1, n):\n",
    "        y4[t] = 0.5 * np.tanh(y3[t-1]) + np.random.laplace()\n",
    "\n",
    "    # Generate y5 influenced by latent2 and itself (Non-linear and Non-Gaussian)\n",
    "    y5[0] = np.random.laplace()\n",
    "    for t in range(1, n):\n",
    "        y5[t] = np.tanh(0.7 * y5[t-1]) + 0.5 * np.log(np.abs(latent2[t]) + 1) + np.random.laplace()\n",
    "\n",
    "    # Introduce NaN values randomly\n",
    "    nan_indices = np.random.choice(n, size=int(n * nan_ratio), replace=False)\n",
    "    for y in [y1, y2, y3, y4, y5]:\n",
    "        y[nan_indices] = np.nan\n",
    "\n",
    "    # Combine into DataFrame\n",
    "    data = pd.DataFrame({'y1': y1, 'y2': y2, 'y3': y3, 'y4': y4, 'y5': y5})\n",
    "\n",
    "    # Ground truth causal matrix\n",
    "    ground_truth = np.array([\n",
    "        [['', '-->'], ['o-o', ''], ['', ''], ['', ''], ['', '']],\n",
    "        [['o-o', ''], ['', '-->'], ['-->', ''], ['', ''], ['', '']],\n",
    "        [['', ''], ['<--', ''], ['', '-->'], ['', '-->'], ['o-o', '']],\n",
    "        [['', ''], ['', ''], ['', '<--'], ['', ''], ['', '']],\n",
    "        [['', ''], ['', ''], ['o-o', ''], ['', ''], ['', '-->']]\n",
    "    ])\n",
    "\n",
    "    return data, ground_truth\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "10 nodes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "         y1        y2        y3        y4        y5        y6        y7  \\\n",
      "0  0.262625  0.439522 -0.514745  0.681817 -0.941773  1.321716  0.761944   \n",
      "1 -0.630616 -1.473605 -0.507248 -1.417785 -0.720644 -2.729940 -0.747854   \n",
      "2 -2.128621 -0.627970  0.329550 -1.375481 -0.660218  0.706367 -0.454569   \n",
      "3 -0.748819 -0.968439 -0.809640 -0.685816  1.057344 -0.308763 -1.010588   \n",
      "4  1.861781  1.172340  0.175683 -0.547207  1.418841  1.944589 -1.533666   \n",
      "\n",
      "         y8        y9       y10  \n",
      "0  0.894747  0.542049  0.637034  \n",
      "1  0.921450 -1.897056  1.450709  \n",
      "2 -1.217333 -0.721907 -1.789402  \n",
      "3  0.564964 -0.197513 -0.758310  \n",
      "4 -0.146895 -1.126502 -0.678719  \n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(          y1        y2        y3        y4        y5        y6        y7  \\\n",
       " 0   0.262625  0.439522 -0.514745  0.681817 -0.941773  1.321716  0.761944   \n",
       " 1  -0.630616 -1.473605 -0.507248 -1.417785 -0.720644 -2.729940 -0.747854   \n",
       " 2  -2.128621 -0.627970  0.329550 -1.375481 -0.660218  0.706367 -0.454569   \n",
       " 3  -0.748819 -0.968439 -0.809640 -0.685816  1.057344 -0.308763 -1.010588   \n",
       " 4   1.861781  1.172340  0.175683 -0.547207  1.418841  1.944589 -1.533666   \n",
       " 5   0.224707 -1.207436  0.535360  2.279898  0.096602  0.034396 -0.396807   \n",
       " 6  -0.689758 -1.292714  1.040664  1.524622 -2.251377 -0.823914 -2.009425   \n",
       " 7   0.249865 -0.361779 -0.339004  0.314539  1.130195 -0.617462  0.064076   \n",
       " 8   2.500232  2.618285  1.514138  0.449225  2.415574  0.248001 -1.964231   \n",
       " 9   1.441422  1.452818  2.251915  2.795177  1.616762  2.974181  3.800008   \n",
       " 10  0.049156 -0.022579  1.705225  0.429595  2.388770 -1.366967  0.155301   \n",
       " 11 -1.054129 -0.577903 -1.199180 -1.205905  0.765448  2.363444  1.525745   \n",
       " 12  0.394109  0.587163 -1.273971  1.635394  1.596448  0.007704 -1.595688   \n",
       " 13  1.563548 -0.157978  0.790822 -1.161317  1.759493  1.979174  0.541409   \n",
       " 14 -0.581784 -0.292514  0.304665 -1.054223  1.867390  1.514668  0.405361   \n",
       " 15  0.491310  0.147365 -1.098784 -0.516250  0.928736  0.502120 -1.315889   \n",
       " 16  0.755886 -0.673889  1.168830 -0.288651  0.043689  0.458234 -0.083939   \n",
       " 17  1.243894  0.062520  0.426985  0.623484 -0.303049 -1.293710 -0.531940   \n",
       " 18 -1.023541  0.377835 -0.005605 -0.806352 -0.205934 -1.452700 -0.104624   \n",
       " 19 -0.254712 -0.178666  0.122392  0.445705  1.182430 -0.279947 -0.139391   \n",
       " 20 -2.602445 -1.856190  1.184533  1.844892 -0.534127 -0.346076 -0.439073   \n",
       " 21 -0.887979 -0.231689 -0.594617 -0.098219 -1.547147 -0.694319 -1.294753   \n",
       " 22       NaN       NaN       NaN       NaN       NaN       NaN       NaN   \n",
       " 23  1.262912  0.482164 -0.235258  0.956660  1.114166 -2.408147  0.781871   \n",
       " 24  1.456100  1.146345  3.055850  4.788550  0.803766  2.804561 -0.032727   \n",
       " 25 -0.515162 -0.855429  0.295946 -1.033755  1.231741  0.464404  1.143278   \n",
       " 26 -0.062836  1.668768 -0.467194 -1.147460 -0.448031  3.001846  0.924572   \n",
       " 27  1.165459  1.443424 -0.561646 -0.542337 -0.074786 -1.963806  0.152991   \n",
       " 28  0.202380  0.452766  1.012699  0.831677  0.168405  0.617109  2.483482   \n",
       " 29       NaN       NaN       NaN       NaN       NaN       NaN       NaN   \n",
       " 30       NaN       NaN       NaN       NaN       NaN       NaN       NaN   \n",
       " 31  1.926865  0.422428  0.497664  0.655358 -0.586158  0.328265 -0.633629   \n",
       " 32       NaN       NaN       NaN       NaN       NaN       NaN       NaN   \n",
       " 33  0.663352  0.051807  0.564511  0.477393 -0.603037 -0.377586 -1.601678   \n",
       " 34  0.805397  0.492463  0.785537 -1.156958  0.597287 -1.007536  1.304436   \n",
       " 35 -0.900190 -0.058885  0.267864  0.310473 -0.664663  0.197351  1.758270   \n",
       " 36 -2.802578 -1.895725  0.649141 -0.703221 -1.509427 -2.133560 -0.227688   \n",
       " 37  0.396841 -1.905698 -2.055263 -2.270696 -2.747002 -3.584228 -2.386345   \n",
       " 38       NaN       NaN       NaN       NaN       NaN       NaN       NaN   \n",
       " 39 -0.323606 -0.708831  1.346521  0.656566 -2.317808 -2.912911 -2.198862   \n",
       " 40 -2.022938 -2.023946 -0.277211 -1.777610 -5.554983 -1.992638 -0.526811   \n",
       " 41  1.157644 -1.997469 -0.802704 -1.640650 -3.399496 -5.989311 -1.457518   \n",
       " 42 -1.501285 -1.312089  1.104028  0.576758 -2.931236 -4.574541 -2.308102   \n",
       " 43 -0.696441 -2.164539 -1.454281 -1.557635 -4.360991 -2.125764 -3.803839   \n",
       " 44 -0.264006 -0.114554 -0.907275 -1.084126 -2.301313 -3.035044 -3.523559   \n",
       " 45  1.575574  2.952150  0.351286  1.364711 -0.522133 -1.859813 -0.949947   \n",
       " 46  0.021089  0.033426  0.951450  2.451510  0.106846  1.378618  0.650100   \n",
       " 47 -2.236697 -1.522585  0.134339  1.633365  0.352991  0.874932  1.557068   \n",
       " 48  0.400823  0.844782 -1.793677 -2.985847  0.824115 -0.115222 -1.634042   \n",
       " 49  0.396920 -0.266216 -1.024299 -1.386084 -0.275568  0.435417 -0.178814   \n",
       " \n",
       "           y8        y9       y10  \n",
       " 0   0.894747  0.542049  0.637034  \n",
       " 1   0.921450 -1.897056  1.450709  \n",
       " 2  -1.217333 -0.721907 -1.789402  \n",
       " 3   0.564964 -0.197513 -0.758310  \n",
       " 4  -0.146895 -1.126502 -0.678719  \n",
       " 5   0.678536 -0.381085 -1.236777  \n",
       " 6  -0.911894  0.322832  0.402389  \n",
       " 7   3.029537  2.209798  1.283421  \n",
       " 8  -2.102696  1.766022  2.105567  \n",
       " 9   3.571631  4.578027  1.283310  \n",
       " 10  1.752814  1.490561  4.868846  \n",
       " 11  2.265197 -1.873285  3.729101  \n",
       " 12 -1.974744  2.641487 -2.490302  \n",
       " 13 -1.607796  2.161903  4.311635  \n",
       " 14  0.567283 -1.704282  2.654549  \n",
       " 15 -1.053529  0.410782 -2.291084  \n",
       " 16 -0.225221 -1.521324  0.306785  \n",
       " 17  0.590677 -0.210848 -0.679815  \n",
       " 18  0.562340 -1.545996 -0.833788  \n",
       " 19 -0.179433  1.046036 -1.005581  \n",
       " 20 -0.662887  0.976012  0.825128  \n",
       " 21 -0.391544 -1.740931 -0.440759  \n",
       " 22       NaN       NaN       NaN  \n",
       " 23  0.327174 -0.936085 -0.664783  \n",
       " 24 -0.308282  0.491691  0.929862  \n",
       " 25  3.018883  0.786573  1.900791  \n",
       " 26 -0.717653  0.928354  1.731539  \n",
       " 27  1.670614  0.058034  1.859559  \n",
       " 28  2.113874 -0.056728  1.682996  \n",
       " 29       NaN       NaN       NaN  \n",
       " 30       NaN       NaN       NaN  \n",
       " 31  1.252650  0.767637  0.150019  \n",
       " 32       NaN       NaN       NaN  \n",
       " 33  1.431792  0.844148  1.276498  \n",
       " 34  1.548148 -0.625712  2.543036  \n",
       " 35  3.329087 -1.921301  0.451850  \n",
       " 36  1.244317 -1.372385  0.713659  \n",
       " 37 -0.217020 -2.960341 -1.350305  \n",
       " 38       NaN       NaN       NaN  \n",
       " 39 -1.358137 -0.875712 -2.306391  \n",
       " 40  0.157060 -2.024124  0.677394  \n",
       " 41 -2.091931 -2.833440 -1.819540  \n",
       " 42 -1.235287 -0.489117 -1.000709  \n",
       " 43 -0.784340  0.236900 -1.533764  \n",
       " 44 -2.899191  1.114804 -0.943656  \n",
       " 45  0.052729  2.051491 -0.154528  \n",
       " 46  0.710430  1.627239 -0.366718  \n",
       " 47  3.381722 -1.137527  3.276334  \n",
       " 48 -0.110801 -1.566866  1.677837  \n",
       " 49 -2.108558 -1.040437 -3.867661  ,\n",
       " array([[['', ''],\n",
       "         ['<--', ''],\n",
       "         ['', '-->'],\n",
       "         ['', ''],\n",
       "         ['', ''],\n",
       "         ['', ''],\n",
       "         ['', ''],\n",
       "         ['', ''],\n",
       "         ['', ''],\n",
       "         ['', '']],\n",
       " \n",
       "        [['-->', ''],\n",
       "         ['', ''],\n",
       "         ['', ''],\n",
       "         ['', '-->'],\n",
       "         ['o-o', ''],\n",
       "         ['o-o', ''],\n",
       "         ['', '-->'],\n",
       "         ['', ''],\n",
       "         ['o-o', ''],\n",
       "         ['', '']],\n",
       " \n",
       "        [['', '<--'],\n",
       "         ['', ''],\n",
       "         ['', ''],\n",
       "         ['-->', ''],\n",
       "         ['', ''],\n",
       "         ['', ''],\n",
       "         ['', ''],\n",
       "         ['', '-->'],\n",
       "         ['', ''],\n",
       "         ['', '']],\n",
       " \n",
       "        [['', ''],\n",
       "         ['', '<--'],\n",
       "         ['<--', ''],\n",
       "         ['', ''],\n",
       "         ['', ''],\n",
       "         ['', ''],\n",
       "         ['', ''],\n",
       "         ['', ''],\n",
       "         ['-->', ''],\n",
       "         ['', '']],\n",
       " \n",
       "        [['', ''],\n",
       "         ['o-o', ''],\n",
       "         ['', ''],\n",
       "         ['', ''],\n",
       "         ['', '-->'],\n",
       "         ['', '-->'],\n",
       "         ['', ''],\n",
       "         ['', ''],\n",
       "         ['o-o', ''],\n",
       "         ['', '']],\n",
       " \n",
       "        [['', ''],\n",
       "         ['o-o', ''],\n",
       "         ['', ''],\n",
       "         ['', ''],\n",
       "         ['', '<--'],\n",
       "         ['', ''],\n",
       "         ['-->', ''],\n",
       "         ['', ''],\n",
       "         ['o-o', ''],\n",
       "         ['', '']],\n",
       " \n",
       "        [['', ''],\n",
       "         ['', '<--'],\n",
       "         ['', ''],\n",
       "         ['', ''],\n",
       "         ['', ''],\n",
       "         ['<--', ''],\n",
       "         ['', ''],\n",
       "         ['-->', ''],\n",
       "         ['', ''],\n",
       "         ['', '']],\n",
       " \n",
       "        [['', ''],\n",
       "         ['', ''],\n",
       "         ['', '<--'],\n",
       "         ['', ''],\n",
       "         ['', ''],\n",
       "         ['', ''],\n",
       "         ['<--', ''],\n",
       "         ['', ''],\n",
       "         ['', ''],\n",
       "         ['-->', '']],\n",
       " \n",
       "        [['', ''],\n",
       "         ['o-o', ''],\n",
       "         ['', ''],\n",
       "         ['<--', ''],\n",
       "         ['o-o', ''],\n",
       "         ['o-o', ''],\n",
       "         ['', ''],\n",
       "         ['', ''],\n",
       "         ['', ''],\n",
       "         ['', '-->']],\n",
       " \n",
       "        [['', ''],\n",
       "         ['', ''],\n",
       "         ['', ''],\n",
       "         ['', ''],\n",
       "         ['', ''],\n",
       "         ['', ''],\n",
       "         ['', ''],\n",
       "         ['<--', ''],\n",
       "         ['', '<--'],\n",
       "         ['', '']]], dtype='<U3'))"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# Function to generate the dataset and its ground truth causal matrix for a 10-node system\n",
    "def generate_causal_dataset(n, nan_ratio=0.1, phi_latent=0.8):\n",
    "    # Initialize latent confounder\n",
    "    latent1 = np.zeros(n)\n",
    "    latent1[0] = np.random.normal()\n",
    "\n",
    "    # Generate latent confounder using an AR(1) process\n",
    "    for t in range(1, n):\n",
    "        latent1[t] = phi_latent * latent1[t - 1] + np.random.normal()\n",
    "\n",
    "    # Initialize variables y1 to y10\n",
    "    y = {f'y{i}': np.zeros(n) for i in range(1, 11)}\n",
    "\n",
    "    # Define the causal relationships\n",
    "    # y2 influenced by latent1\n",
    "    y['y2'][0] = np.random.normal()\n",
    "    for t in range(1, n):\n",
    "        y['y2'][t] = 0.6 * latent1[t] + np.random.normal()\n",
    "\n",
    "    # y1 influenced by y2 (contemporaneous)\n",
    "    y['y1'][0] = np.random.normal()\n",
    "    for t in range(1, n):\n",
    "        y['y1'][t] = 0.5 * y['y2'][t] + np.random.normal()\n",
    "\n",
    "    # y3 influenced by y1[t-1]\n",
    "    y['y3'][0] = np.random.normal()\n",
    "    for t in range(1, n):\n",
    "        y['y3'][t] = 0.7 * y['y1'][t - 1] + np.random.normal()\n",
    "\n",
    "    # y4 influenced by y2[t-1] and y3[t] (contemporaneous)\n",
    "    y['y4'][0] = np.random.normal()\n",
    "    for t in range(1, n):\n",
    "        y['y4'][t] = 0.5 * y['y2'][t - 1] + 0.8 * y['y3'][t] + np.random.normal()\n",
    "\n",
    "    # y5 influenced by latent1 and self (y5[t-1])\n",
    "    y['y5'][0] = np.random.normal()\n",
    "    for t in range(1, n):\n",
    "        y['y5'][t] = 0.6 * latent1[t] + 0.5 * y['y5'][t - 1] + np.random.normal()\n",
    "\n",
    "    # Additional nodes y6 to y10 with new relationships\n",
    "    # y6 influenced by y5[t-1] and latent1\n",
    "    y['y6'][0] = np.random.normal()\n",
    "    for t in range(1, n):\n",
    "        y['y6'][t] = 0.7 * y['y5'][t - 1] + 0.4 * latent1[t] + np.random.normal()\n",
    "\n",
    "    # y7 influenced by y6[t] and y2[t-1]\n",
    "    y['y7'][0] = np.random.normal()\n",
    "    for t in range(1, n):\n",
    "        y['y7'][t] = 0.5 * y['y6'][t] + 0.3 * y['y2'][t - 1] + np.random.normal()\n",
    "\n",
    "    # y8 influenced by y3[t-1] and y7[t]\n",
    "    y['y8'][0] = np.random.normal()\n",
    "    for t in range(1, n):\n",
    "        y['y8'][t] = 0.6 * y['y3'][t - 1] + 0.5 * y['y7'][t] + np.random.normal()\n",
    "\n",
    "    # y9 influenced by y4[t] and latent1\n",
    "    y['y9'][0] = np.random.normal()\n",
    "    for t in range(1, n):\n",
    "        y['y9'][t] = 0.4 * y['y4'][t] + 0.6 * latent1[t] + np.random.normal()\n",
    "\n",
    "    # y10 influenced by y8[t] and y9[t-1]\n",
    "    y['y10'][0] = np.random.normal()\n",
    "    for t in range(1, n):\n",
    "        y['y10'][t] = 0.5 * y['y8'][t] + 0.7 * y['y9'][t - 1] + np.random.normal()\n",
    "\n",
    "    # Introduce NaN values randomly\n",
    "    nan_indices = np.random.choice(n, size=int(n * nan_ratio), replace=False)\n",
    "    for i in range(1, 11):\n",
    "        y[f'y{i}'][nan_indices] = np.nan\n",
    "\n",
    "    # Combine into DataFrame\n",
    "    data = pd.DataFrame(y)\n",
    "\n",
    "    # Define ground truth causal matrix (updated for 10 nodes)\n",
    "    ground_truth = np.array([\n",
    "        [['', ''], ['<--', ''], ['', '-->'], ['', ''], ['', ''], ['', ''], ['', ''], ['', ''], ['', ''], ['', '']],\n",
    "        [['-->', ''], ['', ''], ['', ''], ['', '-->'], ['o-o', ''], ['o-o', ''], ['', '-->'], ['', ''], ['o-o', ''], ['', '']],\n",
    "        [['', '<--'], ['', ''], ['', ''], ['-->', ''], ['', ''], ['', ''], ['', ''], ['', '-->'], ['', ''], ['', '']],\n",
    "        [['', ''], ['', '<--'], ['<--', ''], ['', ''], ['', ''], ['', ''], ['', ''], ['', ''], ['-->', ''], ['', '']],\n",
    "        [['', ''], ['o-o', ''], ['', ''], ['', ''], ['', '-->'], ['', '-->'], ['', ''], ['', ''], ['o-o', ''], ['', '']],\n",
    "        [['', ''], ['o-o', ''], ['', ''], ['', ''], ['', '<--'], ['', ''], ['-->', ''], ['', ''], ['o-o', ''], ['', '']],\n",
    "        [['', ''], ['', '<--'], ['', ''], ['', ''], ['', ''], ['<--', ''], ['', ''], ['-->', ''], ['', ''], ['', '']],\n",
    "        [['', ''], ['', ''], ['', '<--'], ['', ''], ['', ''], ['', ''], ['<--', ''], ['', ''], ['', ''], ['-->', '']],\n",
    "        [['', ''], ['o-o', ''], ['', ''], ['<--', ''], ['o-o', ''], ['o-o', ''], ['', ''], ['', ''], ['', ''], ['', '-->']],\n",
    "        [['', ''], ['', ''], ['', ''], ['', ''], ['', ''], ['', ''], ['', ''], ['<--', ''], ['', '<--'], ['', '']]\n",
    "    ])\n",
    "\n",
    "    return data, ground_truth\n",
    "\n",
    "generate_causal_dataset(50)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "15 nodes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# Function to generate the dataset and its ground truth causal matrix for a 15-node system\n",
    "def generate_causal_dataset(n, nan_ratio=0.1, phi_latent=0.8):\n",
    "    # Initialize latent confounder\n",
    "    latent1 = np.zeros(n)\n",
    "    latent1[0] = np.random.normal()\n",
    "\n",
    "    # Generate latent confounder using an AR(1) process\n",
    "    for t in range(1, n):\n",
    "        latent1[t] = phi_latent * latent1[t - 1] + np.random.normal()\n",
    "\n",
    "    # Initialize variables y1 to y15\n",
    "    y = {f'y{i}': np.zeros(n) for i in range(1, 16)}\n",
    "\n",
    "    # Define the causal relationships\n",
    "    # y2 influenced by latent1\n",
    "    y['y2'][0] = np.random.normal()\n",
    "    for t in range(1, n):\n",
    "        y['y2'][t] = 0.6 * latent1[t] + np.random.normal()\n",
    "\n",
    "    # y1 influenced by y2 (contemporaneous)\n",
    "    y['y1'][0] = np.random.normal()\n",
    "    for t in range(1, n):\n",
    "        y['y1'][t] = 0.5 * y['y2'][t] + np.random.normal()\n",
    "\n",
    "    # y3 influenced by y1[t-1]\n",
    "    y['y3'][0] = np.random.normal()\n",
    "    for t in range(1, n):\n",
    "        y['y3'][t] = 0.7 * y['y1'][t - 1] + np.random.normal()\n",
    "\n",
    "    # y4 influenced by y2[t-1] and y3[t] (contemporaneous)\n",
    "    y['y4'][0] = np.random.normal()\n",
    "    for t in range(1, n):\n",
    "        y['y4'][t] = 0.5 * y['y2'][t - 1] + 0.8 * y['y3'][t] + np.random.normal()\n",
    "\n",
    "    # y5 influenced by latent1 and self (y5[t-1])\n",
    "    y['y5'][0] = np.random.normal()\n",
    "    for t in range(1, n):\n",
    "        y['y5'][t] = 0.6 * latent1[t] + 0.5 * y['y5'][t - 1] + np.random.normal()\n",
    "\n",
    "    # Additional nodes y6 to y15 with new relationships\n",
    "    # y6 influenced by y5[t-1] and latent1\n",
    "    y['y6'][0] = np.random.normal()\n",
    "    for t in range(1, n):\n",
    "        y['y6'][t] = 0.7 * y['y5'][t - 1] + 0.4 * latent1[t] + np.random.normal()\n",
    "\n",
    "    # y7 influenced by y6[t] and y2[t-1]\n",
    "    y['y7'][0] = np.random.normal()\n",
    "    for t in range(1, n):\n",
    "        y['y7'][t] = 0.5 * y['y6'][t] + 0.3 * y['y2'][t - 1] + np.random.normal()\n",
    "\n",
    "    # y8 influenced by y3[t-1] and y7[t]\n",
    "    y['y8'][0] = np.random.normal()\n",
    "    for t in range(1, n):\n",
    "        y['y8'][t] = 0.6 * y['y3'][t - 1] + 0.5 * y['y7'][t] + np.random.normal()\n",
    "\n",
    "    # y9 influenced by y4[t] and latent1\n",
    "    y['y9'][0] = np.random.normal()\n",
    "    for t in range(1, n):\n",
    "        y['y9'][t] = 0.4 * y['y4'][t] + 0.6 * latent1[t] + np.random.normal()\n",
    "\n",
    "    # y10 influenced by y8[t] and y9[t-1]\n",
    "    y['y10'][0] = np.random.normal()\n",
    "    for t in range(1, n):\n",
    "        y['y10'][t] = 0.5 * y['y8'][t] + 0.7 * y['y9'][t - 1] + np.random.normal()\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    # y11 influenced by y9 and latent1\n",
    "    y['y11'][0] = np.random.normal()\n",
    "    for t in range(1, n):\n",
    "        y['y11'][t] = 0.3 * y['y9'][t] + 0.5 * latent1[t] + np.random.normal()\n",
    "\n",
    "    # y12 influenced by y10[t-1] and y5[t]\n",
    "    y['y12'][0] = np.random.normal()\n",
    "    for t in range(1, n):\n",
    "        y['y12'][t] = 0.4 * y['y10'][t - 1] + 0.6 * y['y5'][t] + np.random.normal()\n",
    "\n",
    "    # y13 influenced by y12 and y7[t-1]\n",
    "    y['y13'][0] = np.random.normal()\n",
    "    for t in range(1, n):\n",
    "        y['y13'][t] = 0.5 * y['y12'][t] + 0.3 * y['y7'][t - 1] + np.random.normal()\n",
    "\n",
    "    # y14 influenced by y8[t] and y13[t-1]\n",
    "    y['y14'][0] = np.random.normal()\n",
    "    for t in range(1, n):\n",
    "        y['y14'][t] = 0.6 * y['y8'][t] + 0.4 * y['y13'][t - 1] + np.random.normal()\n",
    "\n",
    "    # y15 influenced by latent1 and y14[t]\n",
    "    y['y15'][0] = np.random.normal()\n",
    "    for t in range(1, n):\n",
    "        y['y15'][t] = 0.7 * latent1[t] + 0.3 * y['y14'][t] + np.random.normal()\n",
    "\n",
    "    # Introduce NaN values randomly\n",
    "    nan_indices = np.random.choice(n, size=int(n * nan_ratio), replace=False)\n",
    "    for i in range(1, 16):\n",
    "        y[f'y{i}'][nan_indices] = np.nan\n",
    "\n",
    "    # Combine into DataFrame\n",
    "    data = pd.DataFrame(y)\n",
    "\n",
    "    # Define ground truth causal matrix (updated for 15 nodes)\n",
    "    ground_truth = np.array([\n",
    "        [['', ''], ['<--', ''], ['', '-->'], ['', ''], ['', ''], ['', ''], ['', ''], ['', ''], ['', ''], ['', ''], ['', ''], ['', ''], ['', ''], ['', ''], ['', '']],\n",
    "        [['-->', ''], ['', ''], ['', ''], ['', '-->'], ['o-o', ''], ['o-o', ''], ['', '-->'], ['', ''], ['o-o', ''], ['', ''], ['o-o', ''], ['', ''], ['', ''], ['', ''], ['o-o', '']],\n",
    "        [['', '<--'], ['', ''], ['', ''], ['-->', ''], ['', ''], ['', ''], ['', ''], ['', '-->'], ['', ''], ['', ''], ['', ''], ['', ''], ['', ''], ['', ''], ['', '']],\n",
    "        [['', ''], ['', '<--'], ['<--', ''], ['', ''], ['', ''], ['', ''], ['', ''], ['', ''], ['-->', ''], ['', ''], ['', ''], ['', ''], ['', ''], ['', ''], ['', '']],\n",
    "        [['', ''], ['o-o', ''], ['', ''], ['', ''], ['', '-->'], ['', '-->'], ['', ''], ['', ''], ['o-o', ''], ['', ''], ['o-o', ''], ['-->', ''], ['', ''], ['', ''], ['o-o', '']],\n",
    "        [['', ''], ['o-o', ''], ['', ''], ['', ''], ['', '<--'], ['', ''], ['-->', ''], ['', ''], ['o-o', ''], ['', ''], ['o-o', ''], ['', ''], ['', ''], ['', ''], ['o-o', '']],\n",
    "        [['', ''], ['', '<--'], ['', ''], ['', ''], ['', ''], ['<--', ''], ['', ''], ['-->', ''], ['', ''], ['', ''], ['', ''], ['', '-->'], ['', ''], ['', ''], ['', '']],\n",
    "        [['', ''], ['', ''], ['', '<--'], ['', ''], ['', ''], ['', ''], ['<--', ''], ['', ''], ['', ''], ['-->', ''], ['', ''], ['', ''], ['', ''], ['-->', ''], ['', '']],\n",
    "        [['', ''], ['o-o', ''], ['', ''], ['<--', ''], ['o-o', ''], ['o-o', ''], ['', ''], ['', ''], ['', ''], ['', '-->'], ['-->', ''], ['', ''], ['', ''], ['', ''], ['o-o', '']],\n",
    "        [['', ''], ['', ''], ['', ''], ['', ''], ['', ''], ['', ''], ['', ''], ['<--', ''], ['', '<--'], ['', ''], ['', ''], ['', '-->'], ['', ''], ['', ''], ['', '']],\n",
    "        [['', ''], ['o-o', ''], ['', ''], ['', ''], ['o-o', ''], ['o-o', ''], ['', ''], ['', ''], ['<--', ''], ['', ''], ['', ''], ['', ''], ['', ''], ['', ''], ['o-o', '']],\n",
    "        [['', ''], ['', ''], ['', ''], ['', ''], ['<--', ''], ['', ''], ['', ''], ['', ''], ['', ''], ['', '<--'], ['', ''], ['', ''], ['-->', ''], ['', ''], ['', '']],\n",
    "        [['', ''], ['', ''], ['', ''], ['', ''], ['', ''], ['', ''], ['', '<--'], ['', ''], ['', ''], ['', ''], ['', ''], ['<--', ''], ['', ''], ['', '-->'], ['', '']],\n",
    "        [['', ''], ['', ''], ['', ''], ['', ''], ['', ''], ['', ''], ['', ''], ['<--', ''], ['', ''], ['', ''], ['', ''], ['', ''], ['', '<--'], ['', ''], ['-->', '']],\n",
    "        [['', ''], ['o-o', ''], ['', ''], ['', ''], ['o-o', ''], ['o-o', ''], ['', ''], ['', ''], ['o-o', ''], ['', ''], ['o-o', ''], ['', ''], ['', ''], ['<--', ''], ['', '']]\n",
    "    ])\n",
    "\n",
    "    return data, ground_truth\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
