{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.model_selection import train_test_split\n",
    "from scipy.stats import norm\n",
    "import numpy as np\n",
    "from statsmodels.tsa.stattools import adfuller,kpss\n",
    "import pandas as pd\n",
    "from matplotlib import pyplot as plt\n",
    "from tigramite import data_processing as pp\n",
    "from tigramite.toymodels import structural_causal_processes as toys\n",
    "from tigramite import plotting as tp\n",
    "from tigramite.lpcmci import LPCMCI\n",
    "from tigramite.pcmci import PCMCI\n",
    "from tigramite.independence_tests.parcorr import ParCorr\n",
    "from statsmodels.tools.sm_exceptions import InterpolationWarning\n",
    "from tigramite.independence_tests.cmiknn import CMIknn\n",
    "from tigramite.lpcmci import LPCMCI\n",
    "import seaborn as sns\n",
    "from tqdm import tqdm\n",
    "import time\n",
    "from sklearn.model_selection import RandomizedSearchCV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    Data  Elapsed Time (s)  F1 Score       CMI  Recall\n",
      "0    500         94.655152  0.000000  0.291472     0.0\n",
      "1    500        100.808934  0.000000  0.334062     0.0\n",
      "2    500        124.678295  0.000000  0.408666     0.0\n",
      "3    500        302.720707  0.000000  0.242574     0.0\n",
      "4    500         78.231628  0.000000  0.355476     0.0\n",
      "5    500         72.866141  0.000000  0.312095     0.0\n",
      "6    500        104.366418  0.000000  0.309530     0.0\n",
      "7    500         79.696465  0.000000  0.433259     0.0\n",
      "8    500        121.020400  0.000000  0.334345     0.0\n",
      "9    500        187.937017  0.000000  0.262773     0.0\n",
      "10   500       1106.812213  0.160000  0.237765     0.0\n",
      "11   500       1238.472184  0.000000  0.232632     0.0\n",
      "12   500        918.151631  0.090909  0.265011     0.0\n",
      "13   500        660.727423  0.090909  0.281807     0.0\n",
      "14   500        683.547585  0.000000  0.228636     0.0\n",
      "15   500        523.312134  0.095238  0.259721     0.0\n",
      "16   500       1327.087492  0.160000  0.202110     0.0\n",
      "17   500       1131.440494  0.000000  0.201718     0.0\n",
      "18   500       1014.501928  0.000000  0.230153     0.0\n",
      "19   500       1206.329295  0.000000  0.195928     0.0\n",
      "20   500        994.457309  0.363636  0.311713     0.8\n",
      "21   500        895.388710  0.347826  0.313774     0.0\n",
      "22   500        942.229353  0.538462  0.325966     0.4\n",
      "23   500        905.086350  0.347826  0.362095     0.0\n",
      "24   500        866.833991  0.285714  0.387134     0.0\n",
      "25   500        805.892575  0.347826  0.390497     0.0\n",
      "26   500        759.519424  0.380952  0.245304     0.0\n",
      "27   500        803.097130  0.347826  0.271922     0.0\n",
      "28   500       1246.685436  0.454545  0.340462     0.8\n",
      "29   500        924.736117  0.333333  0.256736     0.0\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Define the data for each table\n",
    "data_1 = {\n",
    "    \"Data\": [500, 500, 500, 500, 500, 500, 500, 500, 500, 500],\n",
    "    \"Elapsed Time (s)\": [94.655152, 100.808934, 124.678295, 302.720707, 78.231628, \n",
    "                         72.866141, 104.366418, 79.696465, 121.020400, 187.937017],\n",
    "    \"F1 Score\": [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
    "    \"CMI\": [0.291472, 0.334062, 0.408666, 0.242574, 0.355476, 0.312095, 0.309530, \n",
    "            0.433259, 0.334345, 0.262773],\n",
    "    \"Recall\": [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]\n",
    "}\n",
    "\n",
    "data_2 = {\n",
    "    \"Data\": [500, 500, 500, 500, 500, 500, 500, 500, 500, 500],\n",
    "    \"Elapsed Time (s)\": [1106.812213, 1238.472184, 918.151631, 660.727423, 683.547585, \n",
    "                         523.312134, 1327.087492, 1131.440494, 1014.501928, 1206.329295],\n",
    "    \"F1 Score\": [0.160000, 0.000000, 0.090909, 0.090909, 0.000000, 0.095238, 0.160000, \n",
    "                 0.000000, 0.000000, 0.000000],\n",
    "    \"CMI\": [0.237765, 0.232632, 0.265011, 0.281807, 0.228636, 0.259721, 0.202110, \n",
    "            0.201718, 0.230153, 0.195928],\n",
    "    \"Recall\": [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]\n",
    "}\n",
    "\n",
    "data_3 = {\n",
    "    \"Data\": [500, 500, 500, 500, 500, 500, 500, 500, 500, 500],\n",
    "    \"Elapsed Time (s)\": [994.457309, 895.388710, 942.229353, 905.086350, 866.833991, \n",
    "                         805.892575, 759.519424, 803.097130, 1246.685436, 924.736117],\n",
    "    \"F1 Score\": [0.363636, 0.347826, 0.538462, 0.347826, 0.285714, 0.347826, 0.380952, \n",
    "                 0.347826, 0.454545, 0.333333],\n",
    "    \"CMI\": [0.311713, 0.313774, 0.325966, 0.362095, 0.387134, 0.390497, 0.245304, \n",
    "            0.271922, 0.340462, 0.256736],\n",
    "    \"Recall\": [0.8, 0.0, 0.4, 0.0, 0.0, 0.0, 0.0, 0.0, 0.8, 0.0]\n",
    "}\n",
    "\n",
    "# Create dataframes for each data set\n",
    "gpdc_sim_1_500= pd.DataFrame(data_1)\n",
    "gpdc_sim_2_500 = pd.DataFrame(data_2)\n",
    "gpdc_sim_3_500 = pd.DataFrame(data_3)\n",
    "\n",
    "gpdc_sim_500= pd.concat([gpdc_sim_1_500, gpdc_sim_2_500, gpdc_sim_3_500], ignore_index=True)\n",
    "print(gpdc_sim_500)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    Data  Elapsed Time (s)  F1 Score       CMI  Recall\n",
      "0    500      10129.255918  0.000000  0.171344     0.0\n",
      "1    500      11022.666821  0.173913  0.150193     0.0\n",
      "2    500      12455.431165  0.272727  0.144713     0.0\n",
      "3    500      11581.570691  0.000000  0.112121     0.0\n",
      "4    500      11597.781317  0.000000  0.097986     0.0\n",
      "5    500      24037.441828  0.000000  0.069767     0.0\n",
      "6    500      15202.559369  0.000000  0.094154     0.0\n",
      "7    500      22351.730727  0.000000  0.093658     0.0\n",
      "8    500      11929.633530  0.181818  0.099954     1.0\n",
      "9    500      12268.496743  0.000000  0.075750     0.0\n",
      "10   500      14065.565789  0.000000  0.051424     0.0\n",
      "11   500      13531.277405  0.181818  0.088734     1.0\n",
      "12   500      12317.541187  0.000000  0.099041     0.0\n",
      "13   500      11173.311492  0.000000  0.052929     0.0\n",
      "14   500       8061.846174  0.000000  0.101541     0.0\n",
      "15   500      11658.323375  0.000000  0.115299     0.0\n",
      "16   500      11877.713583  0.086957  0.088808     0.0\n",
      "17   500      12893.179200  0.272727  0.089659     1.0\n",
      "18   500      11986.353827  0.000000  0.095417     0.0\n",
      "19   500      22184.998852  0.000000  0.067896     0.0\n",
      "20   500      14692.095101  0.400000  0.233960     0.0\n",
      "21   500      20347.738058  0.347826  0.161292     0.0\n",
      "22   500      18816.518311  0.111111  0.191972     0.0\n",
      "23   500      14489.055614  0.380952  0.219009     0.0\n",
      "24   500      23965.427058  0.000000  0.270779     0.0\n",
      "25   500      16610.189237  0.380952  0.121520     0.0\n",
      "26   500      10089.255925  0.571429  0.136589     0.4\n",
      "27   500      11705.648939  0.000000  0.182930     0.0\n",
      "28   500      10632.992696  0.476190  0.144942     0.4\n",
      "29   500      10659.989593  0.333333  0.211936     0.0\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Define the data for each table\n",
    "data_1 = {\n",
    "    \"Data\": [500, 500, 500, 500, 500, 500, 500, 500, 500, 500],\n",
    "    \"Elapsed Time (s)\": [10129.255918, 11022.666821, 12455.431165, 11581.570691, 11597.781317, \n",
    "                         24037.441828, 15202.559369, 22351.730727, 11929.633530, 12268.496743],\n",
    "    \"F1 Score\": [0.000000, 0.173913, 0.272727, 0.000000, 0.000000, 0.000000, 0.000000, \n",
    "                 0.000000, 0.181818, 0.000000],\n",
    "    \"CMI\": [0.171344, 0.150193, 0.144713, 0.112121, 0.097986, 0.069767, 0.094154, \n",
    "            0.093658, 0.099954, 0.075750],\n",
    "    \"Recall\": [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0]\n",
    "}\n",
    "\n",
    "data_2 = {\n",
    "    \"Data\": [500, 500, 500, 500, 500, 500, 500, 500, 500, 500],\n",
    "    \"Elapsed Time (s)\": [14065.565789, 13531.277405, 12317.541187, 11173.311492, 8061.846174, \n",
    "                         11658.323375, 11877.713583, 12893.179200, 11986.353827, 22184.998852],\n",
    "    \"F1 Score\": [0.000000, 0.181818, 0.000000, 0.000000, 0.000000, 0.000000, 0.086957, \n",
    "                 0.272727, 0.000000, 0.000000],\n",
    "    \"CMI\": [0.051424, 0.088734, 0.099041, 0.052929, 0.101541, 0.115299, 0.088808, \n",
    "            0.089659, 0.095417, 0.067896],\n",
    "    \"Recall\": [0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0]\n",
    "}\n",
    "\n",
    "data_3 = {\n",
    "    \"Data\": [500, 500, 500, 500, 500, 500, 500, 500, 500, 500],\n",
    "    \"Elapsed Time (s)\": [14692.095101, 20347.738058, 18816.518311, 14489.055614, 23965.427058, \n",
    "                         16610.189237, 10089.255925, 11705.648939, 10632.992696, 10659.989593],\n",
    "    \"F1 Score\": [0.400000, 0.347826, 0.111111, 0.380952, 0.000000, 0.380952, 0.571429, \n",
    "                 0.000000, 0.476190, 0.333333],\n",
    "    \"CMI\": [0.233960, 0.161292, 0.191972, 0.219009, 0.270779, 0.121520, 0.136589, \n",
    "            0.182930, 0.144942, 0.211936],\n",
    "    \"Recall\": [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.4, 0.0, 0.4, 0.0]\n",
    "}\n",
    "\n",
    "# Create dataframes for each data set\n",
    "cmiknn_sim_1_500 = pd.DataFrame(data_1)\n",
    "cmiknn_sim_2_500 = pd.DataFrame(data_2)\n",
    "cmiknn_sim_3_500 = pd.DataFrame(data_3)\n",
    "\n",
    "# Concatenate all dataframes into one\n",
    "cmiknn_sim_500= pd.concat([cmiknn_sim_1_500, cmiknn_sim_2_500, cmiknn_sim_3_500], ignore_index=True)\n",
    "\n",
    "# Display the combined dataframe\n",
    "print(cmiknn_sim_500)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "\"Column(s) ['F1_Score'] do not exist\"",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[6], line 27\u001b[0m\n\u001b[1;32m     25\u001b[0m plt\u001b[38;5;241m.\u001b[39mfigure(figsize\u001b[38;5;241m=\u001b[39m(\u001b[38;5;241m10\u001b[39m, \u001b[38;5;241m6\u001b[39m))\n\u001b[1;32m     26\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m method, df \u001b[38;5;129;01min\u001b[39;00m methods\u001b[38;5;241m.\u001b[39mitems():\n\u001b[0;32m---> 27\u001b[0m     agg_data \u001b[38;5;241m=\u001b[39m \u001b[43mcompute_confidence_interval\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdf\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mF1_Score\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m     28\u001b[0m     plt\u001b[38;5;241m.\u001b[39merrorbar(agg_data\u001b[38;5;241m.\u001b[39mindex, agg_data[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmean\u001b[39m\u001b[38;5;124m'\u001b[39m], yerr\u001b[38;5;241m=\u001b[39magg_data[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mci\u001b[39m\u001b[38;5;124m'\u001b[39m], label\u001b[38;5;241m=\u001b[39mmethod, marker\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mo\u001b[39m\u001b[38;5;124m'\u001b[39m, capsize\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m5\u001b[39m)\n\u001b[1;32m     30\u001b[0m plt\u001b[38;5;241m.\u001b[39mtitle(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mF1 Score with 95\u001b[39m\u001b[38;5;124m%\u001b[39m\u001b[38;5;124m Confidence Interval by Data Input\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "Cell \u001b[0;32mIn[6], line 14\u001b[0m, in \u001b[0;36mcompute_confidence_interval\u001b[0;34m(df, column)\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mcompute_confidence_interval\u001b[39m(df, column):\n\u001b[0;32m---> 14\u001b[0m     agg_df \u001b[38;5;241m=\u001b[39m \u001b[43mdf\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgroupby\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mData\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43magg\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     15\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmean\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mcolumn\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mmean\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     16\u001b[0m \u001b[43m        \u001b[49m\u001b[43mstd\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mcolumn\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mstd\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     17\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcount\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mcolumn\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mcount\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m     18\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     19\u001b[0m     \u001b[38;5;66;03m# Compute standard error and confidence intervals\u001b[39;00m\n\u001b[1;32m     20\u001b[0m     agg_df[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124msem\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m agg_df[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mstd\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m/\u001b[39m np\u001b[38;5;241m.\u001b[39msqrt(agg_df[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcount\u001b[39m\u001b[38;5;124m'\u001b[39m])\n",
      "File \u001b[0;32m~/tigramite/.venv/lib/python3.10/site-packages/pandas/core/groupby/generic.py:895\u001b[0m, in \u001b[0;36mDataFrameGroupBy.aggregate\u001b[0;34m(self, func, engine, engine_kwargs, *args, **kwargs)\u001b[0m\n\u001b[1;32m    892\u001b[0m func \u001b[38;5;241m=\u001b[39m maybe_mangle_lambdas(func)\n\u001b[1;32m    894\u001b[0m op \u001b[38;5;241m=\u001b[39m GroupByApply(\u001b[38;5;28mself\u001b[39m, func, args, kwargs)\n\u001b[0;32m--> 895\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[43mop\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43magg\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    896\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m is_dict_like(func) \u001b[38;5;129;01mand\u001b[39;00m result \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    897\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m result\n",
      "File \u001b[0;32m~/tigramite/.venv/lib/python3.10/site-packages/pandas/core/apply.py:172\u001b[0m, in \u001b[0;36mApply.agg\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    169\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mapply_str()\n\u001b[1;32m    171\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m is_dict_like(arg):\n\u001b[0;32m--> 172\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43magg_dict_like\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    173\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m is_list_like(arg):\n\u001b[1;32m    174\u001b[0m     \u001b[38;5;66;03m# we require a list, but not a 'str'\u001b[39;00m\n\u001b[1;32m    175\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39magg_list_like()\n",
      "File \u001b[0;32m~/tigramite/.venv/lib/python3.10/site-packages/pandas/core/apply.py:496\u001b[0m, in \u001b[0;36mApply.agg_dict_like\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    493\u001b[0m     selected_obj \u001b[38;5;241m=\u001b[39m obj\u001b[38;5;241m.\u001b[39m_selected_obj\n\u001b[1;32m    494\u001b[0m     selection \u001b[38;5;241m=\u001b[39m obj\u001b[38;5;241m.\u001b[39m_selection\n\u001b[0;32m--> 496\u001b[0m arg \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnormalize_dictlike_arg\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43magg\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mselected_obj\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43marg\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    498\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m selected_obj\u001b[38;5;241m.\u001b[39mndim \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m1\u001b[39m:\n\u001b[1;32m    499\u001b[0m     \u001b[38;5;66;03m# key only used for output\u001b[39;00m\n\u001b[1;32m    500\u001b[0m     colg \u001b[38;5;241m=\u001b[39m obj\u001b[38;5;241m.\u001b[39m_gotitem(selection, ndim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m)\n",
      "File \u001b[0;32m~/tigramite/.venv/lib/python3.10/site-packages/pandas/core/apply.py:619\u001b[0m, in \u001b[0;36mApply.normalize_dictlike_arg\u001b[0;34m(self, how, obj, func)\u001b[0m\n\u001b[1;32m    617\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(cols) \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[1;32m    618\u001b[0m         cols_sorted \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlist\u001b[39m(safe_sort(\u001b[38;5;28mlist\u001b[39m(cols)))\n\u001b[0;32m--> 619\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mColumn(s) \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mcols_sorted\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m do not exist\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    621\u001b[0m aggregator_types \u001b[38;5;241m=\u001b[39m (\u001b[38;5;28mlist\u001b[39m, \u001b[38;5;28mtuple\u001b[39m, \u001b[38;5;28mdict\u001b[39m)\n\u001b[1;32m    623\u001b[0m \u001b[38;5;66;03m# if we have a dict of any non-scalars\u001b[39;00m\n\u001b[1;32m    624\u001b[0m \u001b[38;5;66;03m# eg. {'A' : ['mean']}, normalize all to\u001b[39;00m\n\u001b[1;32m    625\u001b[0m \u001b[38;5;66;03m# be list-likes\u001b[39;00m\n\u001b[1;32m    626\u001b[0m \u001b[38;5;66;03m# Cannot use func.values() because arg may be a Series\u001b[39;00m\n",
      "\u001b[0;31mKeyError\u001b[0m: \"Column(s) ['F1_Score'] do not exist\""
     ]
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 1000x600 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy import stats\n",
    "\n",
    "# Combine all data into a dictionary for easy iteration\n",
    "methods = {\n",
    "    'Method 1': cmiknn_sim_500,\n",
    "    'Method 2': gpdc_sim_500,\n",
    "}\n",
    "\n",
    "# Function to compute 95% confidence intervals\n",
    "def compute_confidence_interval(df, column):\n",
    "    agg_df = df.groupby('Data').agg(\n",
    "        mean=(column, 'mean'),\n",
    "        std=(column, 'std'),\n",
    "        count=(column, 'count')\n",
    "    )\n",
    "    # Compute standard error and confidence intervals\n",
    "    agg_df['sem'] = agg_df['std'] / np.sqrt(agg_df['count'])\n",
    "    agg_df['ci'] = 1.96 * agg_df['sem']  # 95% confidence interval\n",
    "    return agg_df\n",
    "\n",
    "# Visualization: F1 Score with 95% CI\n",
    "plt.figure(figsize=(10, 6))\n",
    "for method, df in methods.items():\n",
    "    agg_data = compute_confidence_interval(df, 'F1 core')\n",
    "    plt.errorbar(agg_data.index, agg_data['mean'], yerr=agg_data['ci'], label=method, marker='o', capsize=5)\n",
    "\n",
    "plt.title('F1 Score with 95% Confidence Interval by Data Input')\n",
    "plt.xlabel('Data Input')\n",
    "plt.ylabel('F1 Score')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Visualization: Latent Recall with 95% CI\n",
    "plt.figure(figsize=(10, 6))\n",
    "for method, df in methods.items():\n",
    "    agg_data = compute_confidence_interval(df, 'Latent_Recall')\n",
    "    plt.errorbar(agg_data.index, agg_data['mean'], yerr=agg_data['ci'], label=method, marker='s', capsize=5)\n",
    "\n",
    "plt.title('Latent Recall with 95% Confidence Interval by Data Input')\n",
    "plt.xlabel('Data Input')\n",
    "plt.ylabel('Latent Recall')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Visualization: Runtime with 95% CI\n",
    "plt.figure(figsize=(10, 6))\n",
    "for method, df in methods.items():\n",
    "    agg_data = compute_confidence_interval(df, 'Runtime')\n",
    "    plt.errorbar(agg_data.index, agg_data['mean'], yerr=agg_data['ci'], label=method, marker='d', capsize=5)\n",
    "\n",
    "plt.title('Runtime with 95% Confidence Interval by Data Input')\n",
    "plt.xlabel('Data Input')\n",
    "plt.ylabel('Runtime (seconds)')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# Set random seed for reproducibility\n",
    "np.random.seed(42)\n",
    "\n",
    "# Parameters\n",
    "n = 300  # Length of the time series\n",
    "nan_ratio = .1 # Ratio of NaN values\n",
    "\n",
    "# Generate a latent stationary time series using AR(1) process\n",
    "phi = 0.8  # Autoregressive parameter\n",
    "latent = np.zeros(n)\n",
    "latent[0] = np.random.normal()\n",
    "\n",
    "for t in range(1, n):\n",
    "    latent[t] = phi * latent[t-1] + np.random.normal()\n",
    "\n",
    "# Generate y1 using an AR(1) process with a non-linear transformation\n",
    "y1 = np.zeros(n)\n",
    "y1[0] = np.random.normal()\n",
    "\n",
    "for t in range(1, n):\n",
    "    y1[t] = 0.9 * y1[t-1] + np.random.normal()\n",
    "y1 = np.log(np.abs(y1) + 1) + np.random.normal(scale=0.1, size=n)  # Adding non-linearity\n",
    "\n",
    "# Generate y2 influenced by the latent variable and a non-linear transformation of y1\n",
    "y2 = np.zeros(n)\n",
    "y2[0] = np.random.normal()\n",
    "\n",
    "for t in range(1, n):\n",
    "    y2[t] = 0.5 * latent[t] + 0.3 * np.tanh(y1[t]) + np.random.normal()\n",
    "\n",
    "# Generate y3 influenced by the latent variable and a non-linear transformation of y2\n",
    "y3 = np.zeros(n)\n",
    "y3[0] = np.random.normal()\n",
    "\n",
    "for t in range(1, n):\n",
    "    y3[t] = 0.6 * latent[t] + 0.8 * np.square(y2[t]) + np.random.normal()\n",
    "\n",
    "# Generate y4 influenced by lagged y1 and y3\n",
    "y4 = np.zeros(n)\n",
    "y4[0] = np.random.normal()\n",
    "\n",
    "for t in range(1, n):\n",
    "    y4[t] = 0.4 * np.roll(y1, 1)[t] + 0.9 * np.sqrt(np.abs(y3[t])) + np.random.normal()\n",
    "\n",
    "# Introduce NaN values randomly\n",
    "nan_indices = np.random.choice(n, size=int(n * nan_ratio), replace=False)\n",
    "for y in [y1, y2, y3, y4]:\n",
    "    y[nan_indices] = np.nan\n",
    "\n",
    "# Combine into a DataFrame\n",
    "data = pd.DataFrame({'y0': y1, 'y1': y2, 'y2': y3, 'y3': y4})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_f1_score(ground_truth, predicted):\n",
    "    true_positives = 0\n",
    "    false_positives = 0\n",
    "    false_negatives = 0\n",
    "\n",
    "    equivalences = {\n",
    "    '-->': '-?>',\n",
    "    '<--': '<?-',\n",
    "    'o-o': {'<-o', 'o->', '<->', '<?>', 'o?o'}\n",
    "}\n",
    "    \n",
    "    # Iterate through the arrays and compare relationships\n",
    "    for i in range(ground_truth.shape[0]):\n",
    "        for j in range(ground_truth.shape[1]):\n",
    "            ground_relation = ground_truth[i][j]\n",
    "            predicted_relation = predicted[i][j]\n",
    "            \n",
    "            for t in range(2):  # For time 0 and time lag 1\n",
    "                g_rel = ground_relation[t]\n",
    "                p_rel = predicted_relation[t]\n",
    "                \n",
    "                # Convert any array values to strings for comparison\n",
    "                if isinstance(g_rel, np.ndarray):\n",
    "                    g_rel = str(g_rel)\n",
    "                if isinstance(p_rel, np.ndarray):\n",
    "                    p_rel = str(p_rel)\n",
    "                \n",
    "                if g_rel == '' and p_rel == '':\n",
    "                    # No relation, skip\n",
    "                    continue\n",
    "                elif g_rel == p_rel:\n",
    "                    true_positives += 1\n",
    "                elif g_rel in equivalences:\n",
    "                    # Check if predicted relation matches any of the equivalences\n",
    "                    if p_rel in equivalences[g_rel] if isinstance(equivalences[g_rel], set) else p_rel == equivalences[g_rel]:\n",
    "                        true_positives += 1\n",
    "                    else:\n",
    "                        false_positives += 1\n",
    "                else:\n",
    "                    # If the predicted relation doesn't match the ground truth, it's a false positive\n",
    "                    if p_rel != '':\n",
    "                        false_positives += 1\n",
    "                    # Missed causal link in the predicted array -> false negative\n",
    "                    if g_rel != '':\n",
    "                        false_negatives += 1\n",
    "\n",
    "    # Calculate precision, recall, and F1 score\n",
    "    precision = true_positives / (true_positives + false_positives) if true_positives + false_positives > 0 else 0\n",
    "    recall = true_positives / (true_positives + false_negatives) if true_positives + false_negatives > 0 else 0\n",
    "    f1_score = 2 * precision * recall / (precision + recall) if precision + recall > 0 else 0\n",
    "\n",
    "    return f1_score\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def extract_cmi(graphical_array, values_array):\n",
    "    # List to store conditional mutual information (CMI) values\n",
    "    cmi_values = []\n",
    "    \n",
    "    # Define valid causal links in the graphical array\n",
    "    valid_arrows = {'-->', '<--', 'o-o', '-?>', '<?-', '<->', 'o->', '<-o'}\n",
    "    \n",
    "    # Iterate through both arrays to extract CMI values where there are causal arrows\n",
    "    for i in range(graphical_array.shape[0]):\n",
    "        for j in range(graphical_array.shape[1]):\n",
    "            ground_relation = graphical_array[i][j]\n",
    "            value_pair = values_array[i][j]\n",
    "            \n",
    "            # Iterate through the two elements (time 0 and time lag 1) in ground_relation\n",
    "            for relation, value in zip(ground_relation, value_pair):\n",
    "                # Check if the relationship is a valid causal arrow\n",
    "                if relation in valid_arrows:\n",
    "                    # Only consider finite values (exclude inf and -inf)\n",
    "                    if np.isfinite(value):\n",
    "                        cmi_values.append(value)\n",
    "    \n",
    "    # Compute the average of the collected CMI values\n",
    "    if len(cmi_values) > 0:\n",
    "        avg_cmi = np.mean(cmi_values)\n",
    "    else:\n",
    "        avg_cmi = 0  # No valid values to average\n",
    "    \n",
    "    return avg_cmi\n",
    "\n",
    "\n",
    "def latent_link_recall(ground_truth, predicted):\n",
    "    # Define valid latent causal links\n",
    "    latent_links = {'o-o', '<-o', 'o->', '<->'}\n",
    "    \n",
    "    true_positives = 0\n",
    "    false_negatives = 0\n",
    "    \n",
    "    # Iterate through both arrays to compare latent links\n",
    "    for i in range(ground_truth.shape[0]):\n",
    "        for j in range(ground_truth.shape[1]):\n",
    "            ground_relation = ground_truth[i][j]\n",
    "            predicted_relation = predicted[i][j]\n",
    "            \n",
    "            # Iterate through the two elements (time 0 and time lag 1)\n",
    "            for g_rel, p_rel in zip(ground_relation, predicted_relation):\n",
    "                # Check for latent links in the ground truth\n",
    "                if g_rel in latent_links:\n",
    "                    if p_rel in latent_links:\n",
    "                        true_positives += 1  # Correctly identified latent link\n",
    "                    else:\n",
    "                        false_negatives += 1  # Latent link missed by the prediction\n",
    "    \n",
    "    # Calculate recall\n",
    "    recall = true_positives / (true_positives + false_negatives) if true_positives + false_negatives > 0 else 0\n",
    "    \n",
    "    return recall\n",
    "\n",
    "def feature_selection_lpcmci(data, gt = [], lag=1, test_size=0.2, random_state=42, alpha=0.05, n_shuffles=100):\n",
    "\n",
    "        def create_lagged_features(df, column, lags):\n",
    "            lagged_df = pd.DataFrame()\n",
    "            lagged_df[column] = df[column]  # Include the original unlagged version\n",
    "            for lag in range(1, lags + 1):\n",
    "                lagged_df[f'{column}_lag{lag}'] = df[column].shift(lag)\n",
    "            lagged_df.dropna()\n",
    "            return lagged_df\n",
    "\n",
    "\n",
    "        def shuffle_column(df, column):\n",
    "            shuffled_df = df.copy()\n",
    "            shuffled_df = shuffled_df[[column]]\n",
    "            shuffled_df[column] = np.random.permutation(df[column].values)\n",
    "            return shuffled_df\n",
    "\n",
    "\n",
    "        def evaluate_significance(m_real, m_shuffles, alpha):\n",
    "            mu, sigma = norm.fit(m_shuffles)\n",
    "            p_value = norm.cdf(m_real, mu, sigma)\n",
    "            return p_value < alpha\n",
    "\n",
    "\n",
    "        def calculate_sma(series, window):\n",
    "            return series.rolling(window=window, min_periods=1).mean()\n",
    "\n",
    "\n",
    "        # Initialize the dictionary\n",
    "        num_vars = data.shape[1]\n",
    "        link_assumptions = {j: {(i, -tau): '' for i in range(num_vars) for tau in range(2) if (i, -tau) != (j, 0)} for j in range(num_vars)}\n",
    "\n",
    "        start = time.time()\n",
    "        for target in data.columns:\n",
    "            potential_drivers = [col for col in data.columns]\n",
    "            window = round(len(data) * .2)\n",
    "\n",
    "\n",
    "            sma_baseline = calculate_sma(data[target], window).dropna()\n",
    "            aligned_data = data.iloc[window-1:].reset_index(drop=True)\n",
    "\n",
    "\n",
    "            sma_baseline = sma_baseline.reset_index(drop=True)\n",
    "            y_baseline = aligned_data[target].reset_index(drop=True)\n",
    "\n",
    "\n",
    "            not_nan_index = y_baseline.dropna().index\n",
    "            sma_baseline = sma_baseline.loc[not_nan_index]\n",
    "            y_baseline = y_baseline.dropna()\n",
    "\n",
    "\n",
    "            X_baseline = sma_baseline.to_frame()\n",
    "\n",
    "\n",
    "            X_train, X_test, y_train, y_test = train_test_split(X_baseline, y_baseline, test_size=test_size, random_state=random_state)\n",
    "            baseline_model = RandomForestRegressor(random_state=random_state)\n",
    "            baseline_model.fit(X_train, y_train)\n",
    "            baseline_predictions = baseline_model.predict(X_test)\n",
    "            current_baseline_mse = mean_squared_error(y_test, baseline_predictions)\n",
    "\n",
    "\n",
    "            definite_drivers = []\n",
    "            discovered_drivers = []\n",
    "            definite_non_drivers = []\n",
    "            aligned_data = data.dropna()\n",
    "            combined_features_ = pd.DataFrame()\n",
    "\n",
    "\n",
    "            while potential_drivers:\n",
    "                current_driver = potential_drivers.pop(0)\n",
    "                driver_lagged = create_lagged_features(aligned_data, current_driver, lag).dropna()\n",
    "                best_mse = current_baseline_mse\n",
    "                best_lag = None\n",
    "            \n",
    "                for lag_num in tqdm(range(0, lag + 1), desc=f'Evaluating {current_driver} lags'):\n",
    "                    if lag_num == 0 and current_driver == target:\n",
    "                        continue\n",
    "                    elif lag_num == 0:\n",
    "                        current_driver_lagged = driver_lagged[[current_driver]]\n",
    "                    else:\n",
    "                        current_driver_lagged = driver_lagged[[f'{current_driver}_lag{lag_num}']]\n",
    "                    '''\n",
    "                    combined_features = pd.concat([combined_features_, current_driver_lagged], axis=1)\n",
    "                    target_column = aligned_data[target].loc[combined_features.index]\n",
    "                    X_train, X_test, y_train, y_test = train_test_split(combined_features, target_column, test_size=test_size, random_state=random_state)\n",
    "                    combined_model = RandomForestRegressor(random_state=random_state)\n",
    "                    combined_model.fit(X_train, y_train)\n",
    "                \n",
    "                    combined_predictions = combined_model.predict(X_test)\n",
    "                    combined_mse = mean_squared_error(y_test, combined_predictions)\n",
    "                    param_dist = {\n",
    "                        'n_estimators': [50, 100, 200, 300, 400, 500],\n",
    "                        'max_depth': [None, 10, 20, 30, 40, 50],\n",
    "                        'min_samples_split': [2, 5, 10],\n",
    "                        'min_samples_leaf': [1, 2, 4],\n",
    "                        'max_features': ['auto', 'sqrt', 'log2'],\n",
    "                        'bootstrap': [True, False]\n",
    "                    }\n",
    "                    '''\n",
    "                    param_dist = {\n",
    "                        'n_estimators': [50, 100, 200, 300, 400, 500],\n",
    "                        'max_depth': [None, 10, 20, 30, 40, 50],\n",
    "                        'min_samples_split': [2, 5, 10],\n",
    "                        'min_samples_leaf': [1, 2, 4],\n",
    "                        'max_features': ['sqrt', 'log2'],\n",
    "                        'bootstrap': [True, False]\n",
    "                    }\n",
    "                    # Perform RandomizedSearchCV for hyperparameter tuning\n",
    "                    random_search = RandomizedSearchCV(estimator=RandomForestRegressor(random_state=random_state),\n",
    "                                                    param_distributions=param_dist,\n",
    "                                                    n_iter=100,  # Number of parameter settings sampled\n",
    "                                                    cv=3,  # Number of cross-validation folds\n",
    "                                                    verbose=0,  # Verbosity level\n",
    "                                                    random_state=random_state,\n",
    "                                                    n_jobs=-1)  # Use all available cores\n",
    "\n",
    "                    # Use the combined features to fit the model\n",
    "                    combined_features = pd.concat([combined_features_, current_driver_lagged], axis=1)\n",
    "                    target_column = aligned_data[target].loc[combined_features.index]\n",
    "                    X_train, X_test, y_train, y_test = train_test_split(combined_features, target_column, test_size=test_size, random_state=random_state)\n",
    "\n",
    "                    # Fit the RandomizedSearchCV model to find the best hyperparameters\n",
    "                    random_search.fit(X_train, y_train)\n",
    "\n",
    "                    # Get the best model from the random search\n",
    "                    best_model = random_search.best_estimator_\n",
    "\n",
    "                    # Predict using the best model\n",
    "                    combined_predictions = best_model.predict(X_test)\n",
    "                    combined_mse = mean_squared_error(y_test, combined_predictions)\n",
    "\n",
    "                    \n",
    "                    if combined_mse < best_mse:\n",
    "                        best_mse = combined_mse\n",
    "                        best_lag = lag_num\n",
    "                \n",
    "                if best_lag is not None:\n",
    "                    shuffle_mse = []\n",
    "\n",
    "                    for _ in range(n_shuffles):\n",
    "                        if best_lag == 0:\n",
    "                            shuffled_data = shuffle_column(driver_lagged, current_driver)\n",
    "                        else:\n",
    "                            shuffled_data = shuffle_column(driver_lagged, f'{current_driver}_lag{best_lag}')\n",
    "\n",
    "                        # Use the combined features to fit the model\n",
    "                        shuffled_features = pd.concat([combined_features_, shuffled_data], axis=1)\n",
    "                        X_train_shuffled, X_test_shuffled, y_train_shuffled, y_test_shuffled = train_test_split(shuffled_features, target_column, test_size=test_size, random_state=random_state)\n",
    "\n",
    "                        # Fit the RandomizedSearchCV model to find the best hyperparameters\n",
    "                        random_search.fit(X_train_shuffled, y_train_shuffled)\n",
    "\n",
    "                        # Get the best model from the random search\n",
    "                        best_model = random_search.best_estimator_\n",
    "                        \n",
    "                        # Predict using the best model\n",
    "                        shuffled_predictions = best_model.predict(X_test_shuffled)\n",
    "                        shuffle_mse.append(mean_squared_error(y_test_shuffled, shuffled_predictions))\n",
    "                \n",
    "                    if evaluate_significance(best_mse, shuffle_mse, alpha):\n",
    "                        if best_lag == 0:\n",
    "                            definite_drivers.append(current_driver)\n",
    "                        else:\n",
    "                            definite_drivers.append(f'{current_driver}_lag{best_lag}')\n",
    "                        current_baseline_mse = best_mse\n",
    "                        combined_features_ = pd.concat([combined_features_, current_driver_lagged], axis=1)\n",
    "                    else:\n",
    "                        discovered_drivers.append(current_driver if best_lag == 0 else f'{current_driver}_lag{best_lag}')\n",
    "                else:\n",
    "                    definite_non_drivers.append(current_driver)\n",
    "\n",
    "            print(f\"Target: {target}\")\n",
    "            print(f\"Definite Drivers: {definite_drivers}\")\n",
    "            print(f\"Discovered Drivers: {discovered_drivers}\")\n",
    "            print(f\"Definite Non-Drivers: {definite_non_drivers}\")\n",
    "            print(f\"Final Baseline MSE: {current_baseline_mse}\")\n",
    "\n",
    "            # Update link_assumptions dictionary\n",
    "            target_idx = data.columns.get_loc(target)\n",
    "            for driver in definite_drivers:\n",
    "                if '_lag' in driver:\n",
    "                    driver_name, lag_value = driver.rsplit('_lag', 1)\n",
    "                    driver_idx = data.columns.get_loc(driver_name)\n",
    "                    lag_value = -int(lag_value)\n",
    "                    link_assumptions[target_idx][(driver_idx, lag_value)] = '-->'\n",
    "                    #link_assumptions[driver_idx][(target_idx, lag_value)] = '<--'\n",
    "                else:\n",
    "                    driver_idx = data.columns.get_loc(driver)\n",
    "                    link_assumptions[target_idx][(driver_idx, 0)] = '-->'\n",
    "                    link_assumptions[driver_idx][(target_idx, 0)] = '<--'\n",
    "\n",
    "            for driver in discovered_drivers:\n",
    "                if '_lag' in driver:\n",
    "                    driver_name, lag_value = driver.rsplit('_lag', 1)\n",
    "                    driver_idx = data.columns.get_loc(driver_name)\n",
    "                    lag_value = -int(lag_value)\n",
    "                    link_assumptions[target_idx][(driver_idx, lag_value)] = '-?>'\n",
    "                    #link_assumptions[driver_idx][(target_idx, lag_value)] = '<?-'\n",
    "                else:\n",
    "                    driver_idx = data.columns.get_loc(driver)\n",
    "                    link_assumptions[target_idx][(driver_idx, 0)] = '-?>'\n",
    "                    link_assumptions[driver_idx][(target_idx, 0)] = '<?-'\n",
    "\n",
    "        for target in data.columns:\n",
    "            target_idx = data.columns.get_loc(target)\n",
    "            for driver in discovered_drivers + definite_drivers:\n",
    "                if '_lag' in driver:\n",
    "                    driver_name, lag_value = driver.rsplit('_lag', 1)\n",
    "                    lag_value = -int(lag_value)\n",
    "                    driver_idx = data.columns.get_loc(driver_name)\n",
    "                    if target_idx != driver_idx: \n",
    "                        if link_assumptions[driver_idx][(target_idx, lag_value)] in ['-?>', '-->'] and link_assumptions[target_idx][(driver_idx, 0)] in ['<?-', '<--']:\n",
    "                            link_assumptions[target_idx][(driver_idx, 0)] = 'o?o'\n",
    "                            link_assumptions[driver_idx][(target_idx, 0)] = 'o?o'\n",
    "                else: \n",
    "                    driver_idx = data.columns.get_loc(driver)\n",
    "                    if target_idx != driver_idx: \n",
    "                        if link_assumptions[driver_idx][(target_idx, 0)] in ['-?>', '-->'] and link_assumptions[target_idx][(driver_idx, 0)] in ['<?-', '<--']:\n",
    "                            link_assumptions[target_idx][(driver_idx, 0)] = 'o?o'\n",
    "                            link_assumptions[driver_idx][(target_idx, 0)] = 'o?o'\n",
    "        print(link_assumptions)\n",
    "        n_a_n = np.isnan(data).any(axis=1)\n",
    "        data[n_a_n] = 999\n",
    "        data = data.values\n",
    "        data = pp.DataFrame(data, var_names = [\"y0\", \"y1\", \"y2\", \"y3\", \"y4\"], missing_flag = 999)\n",
    "        cmi_knn = CMIknn(significance='shuffle_test', knn=0.1, shuffle_neighbors=5, transform='ranks', sig_samples=200)\n",
    "        print(\"Starting LPCMCI...\")\n",
    "        lpcmci_loc = LPCMCI(\n",
    "            dataframe=data, \n",
    "            cond_ind_test=cmi_knn,\n",
    "            verbosity=0)\n",
    "        results = lpcmci_loc.run_lpcmci(tau_max=1, pc_alpha=.2, link_assumptions = link_assumptions, n_preliminary_iterations = 0)\n",
    "        end = time.time()\n",
    "        #tp.plot_time_series_graph(graph=results['graph'],\n",
    "        #                        val_matrix=results['val_matrix'])\n",
    "        if len(gt) == 0:\n",
    "            f1 = 0\n",
    "        else:\n",
    "            f1 = compute_f1_score(gt, results['graph'])\n",
    "\n",
    "        cmi_val = extract_cmi(results['graph'], results['val_matrix'])\n",
    "        recall_latent = latent_link_recall(gt, results['graph'])\n",
    "        elapsed_time = end - start\n",
    "        print(\"Finished.\")\n",
    "        return results, elapsed_time, f1, cmi_val, recall_latent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating y0 lags: 100%|██████████| 2/2 [00:04<00:00,  2.18s/it]\n",
      "Evaluating y1 lags: 100%|██████████| 2/2 [00:02<00:00,  1.19s/it]\n",
      "Evaluating y2 lags: 100%|██████████| 2/2 [00:02<00:00,  1.19s/it]\n",
      "Evaluating y3 lags: 100%|██████████| 2/2 [00:02<00:00,  1.06s/it]\n",
      "Evaluating y4 lags: 100%|██████████| 2/2 [00:02<00:00,  1.01s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Target: y0\n",
      "Definite Drivers: ['y0_lag1', 'y4_lag1']\n",
      "Discovered Drivers: ['y1', 'y2', 'y3']\n",
      "Definite Non-Drivers: []\n",
      "Final Baseline MSE: 0.13451461856491256\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating y0 lags: 100%|██████████| 2/2 [00:02<00:00,  1.03s/it]\n",
      "Evaluating y1 lags: 100%|██████████| 2/2 [00:01<00:00,  1.56it/s]\n",
      "Evaluating y2 lags: 100%|██████████| 2/2 [00:02<00:00,  1.16s/it]\n",
      "Evaluating y3 lags: 100%|██████████| 2/2 [00:02<00:00,  1.14s/it]\n",
      "Evaluating y4 lags: 100%|██████████| 2/2 [00:02<00:00,  1.10s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Target: y1\n",
      "Definite Drivers: ['y1_lag1', 'y2']\n",
      "Discovered Drivers: []\n",
      "Definite Non-Drivers: ['y0', 'y3', 'y4']\n",
      "Final Baseline MSE: 1.5894727746314372\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating y0 lags: 100%|██████████| 2/2 [00:02<00:00,  1.12s/it]\n",
      "Evaluating y1 lags: 100%|██████████| 2/2 [00:02<00:00,  1.13s/it]\n",
      "Evaluating y2 lags: 100%|██████████| 2/2 [00:00<00:00,  2.07it/s]\n",
      "Evaluating y3 lags: 100%|██████████| 2/2 [00:01<00:00,  1.03it/s]\n",
      "Evaluating y4 lags: 100%|██████████| 2/2 [00:02<00:00,  1.01s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Target: y2\n",
      "Definite Drivers: ['y1', 'y3', 'y4']\n",
      "Discovered Drivers: ['y0_lag1', 'y2_lag1']\n",
      "Definite Non-Drivers: []\n",
      "Final Baseline MSE: 1.3230238653145523\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating y0 lags: 100%|██████████| 2/2 [00:02<00:00,  1.16s/it]\n",
      "Evaluating y1 lags: 100%|██████████| 2/2 [00:02<00:00,  1.03s/it]\n",
      "Evaluating y2 lags: 100%|██████████| 2/2 [00:02<00:00,  1.09s/it]\n",
      "Evaluating y3 lags: 100%|██████████| 2/2 [00:01<00:00,  1.65it/s]\n",
      "Evaluating y4 lags: 100%|██████████| 2/2 [00:02<00:00,  1.07s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Target: y3\n",
      "Definite Drivers: []\n",
      "Discovered Drivers: ['y0_lag1', 'y1', 'y2_lag1', 'y3_lag1', 'y4_lag1']\n",
      "Definite Non-Drivers: []\n",
      "Final Baseline MSE: 1.243525373045819\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating y0 lags: 100%|██████████| 2/2 [00:02<00:00,  1.03s/it]\n",
      "Evaluating y1 lags: 100%|██████████| 2/2 [00:02<00:00,  1.03s/it]\n",
      "Evaluating y2 lags: 100%|██████████| 2/2 [00:02<00:00,  1.06s/it]\n",
      "Evaluating y3 lags: 100%|██████████| 2/2 [00:02<00:00,  1.10s/it]\n",
      "Evaluating y4 lags: 100%|██████████| 2/2 [00:01<00:00,  1.82it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Target: y4\n",
      "Definite Drivers: ['y1_lag1', 'y2']\n",
      "Discovered Drivers: []\n",
      "Definite Non-Drivers: ['y0', 'y3', 'y4']\n",
      "Final Baseline MSE: 2.2840763340601877\n",
      "{0: {(0, -1): '-->', (1, 0): '-?>', (1, -1): '', (2, 0): '-?>', (2, -1): '', (3, 0): '-?>', (3, -1): '', (4, 0): '', (4, -1): '-->'}, 1: {(0, 0): '<?-', (0, -1): '', (1, -1): '-->', (2, 0): 'o?o', (2, -1): '', (3, 0): '<?-', (3, -1): '', (4, 0): '', (4, -1): ''}, 2: {(0, 0): '<?-', (0, -1): '-?>', (1, 0): 'o?o', (1, -1): '', (2, -1): '-?>', (3, 0): 'o?o', (3, -1): '', (4, 0): '<--', (4, -1): ''}, 3: {(0, 0): '<?-', (0, -1): '-?>', (1, 0): '-?>', (1, -1): '', (2, 0): 'o?o', (2, -1): '-?>', (3, -1): '-?>', (4, 0): '', (4, -1): '-?>'}, 4: {(0, 0): '', (0, -1): '', (1, 0): '', (1, -1): '-->', (2, 0): '-->', (2, -1): '', (3, 0): '', (3, -1): '', (4, -1): ''}}\n",
      "Starting LPCMCI...\n",
      "Finished.\n",
      "   Elapsed Time (s)  F1 Score       CMI  Recall\n",
      "0        704.939127  0.315789  0.064641     0.5\n"
     ]
    }
   ],
   "source": [
    "# Function to generate one dataset and its ground truth causal matrix\n",
    "def generate_dataset(n, nan_ratio, phi=0.8):\n",
    "    # Initialize latent confounder\n",
    "    latent = np.zeros(n)\n",
    "    latent[0] = np.random.normal()\n",
    "    for t in range(1, n):\n",
    "        latent[t] = phi * latent[t-1] + np.random.normal()\n",
    "\n",
    "    # Initialize variables y1 to y5\n",
    "    y1 = np.zeros(n)\n",
    "    y2 = np.zeros(n)\n",
    "    y3 = np.zeros(n)\n",
    "    y4 = np.zeros(n)\n",
    "    y5 = np.zeros(n)\n",
    "\n",
    "    # Generate y1 using AR(1) with non-linear transformation\n",
    "    y1[0] = np.random.normal()\n",
    "    for t in range(1, n):\n",
    "        y1[t] = 0.9 * y1[t-1] + np.random.normal()\n",
    "    y1 = np.log(np.abs(y1) + 1) + np.random.normal(scale=0.1, size=n)\n",
    "\n",
    "    # Generate y2 influenced by latent and y1\n",
    "    y2[0] = np.random.normal()\n",
    "    for t in range(1, n):\n",
    "        y2[t] = 0.8 * latent[t] + 0.3 * np.tanh(y1[t]) + np.random.normal()\n",
    "\n",
    "    # Generate y3 influenced by latent and y2\n",
    "    y3[0] = np.random.normal()\n",
    "    for t in range(1, n):\n",
    "        y3[t] = 0.6 * latent[t] + np.random.normal()\n",
    "\n",
    "    # Generate y4 influenced by lagged y1 and y3\n",
    "    y4[0] = np.random.normal()\n",
    "    for t in range(1, n):\n",
    "        y4[t] = 0.4 * np.roll(y1, 1)[t] + 0.9 * np.sqrt(np.abs(y3[t])) + np.random.normal()\n",
    "\n",
    "    # Generate y5 influenced by latent, y2, and lagged y4\n",
    "    y5[0] = np.random.normal()\n",
    "    for t in range(1, n):\n",
    "        y5[t] = 0.7 * latent[t] + 0.5 * np.log(np.abs(y2[t]) + 1) + 0.6 * np.roll(y4, 1)[t] + np.random.normal()\n",
    "\n",
    "    # Introduce NaN values randomly\n",
    "    nan_indices = np.random.choice(n, size=int(n * nan_ratio), replace=False)\n",
    "    for y in [y1, y2, y3, y4, y5]:\n",
    "        y[nan_indices] = np.nan\n",
    "\n",
    "    # Combine into DataFrame\n",
    "    data = pd.DataFrame({'y0': y1, 'y1': y2, 'y2': y3, 'y3': y4, 'y4': y5})\n",
    "\n",
    "    # Ground truth causal matrix\n",
    "    ground_truth = np.array([\n",
    "        [['', '-->'], ['-->', ''], ['', ''], ['', '-->'], ['', '']],\n",
    "        [['<--', ''], ['', ''], ['o-o', ''], ['', ''], ['-->', '']],\n",
    "        [['', ''], ['o-o', ''], ['', ''], ['', '-->'], ['o-o', '']],\n",
    "        [['', ''], ['', ''], ['', ''], ['', ''], ['', '-->']],\n",
    "        [['', ''], ['<--', ''], ['o-o', ''], ['', ''], ['', '']]\n",
    "    ])\n",
    "\n",
    "    return data, ground_truth\n",
    "\n",
    "# Parameters\n",
    "n = 300 # Length of time series\n",
    "nan_ratio = 0.1  # Ratio of NaN values\n",
    "num_datasets = 1  # Number of datasets\n",
    "\n",
    "elapsed_time = []\n",
    "f1 = []\n",
    "cmi = []\n",
    "recall = []\n",
    "\n",
    "for i in range(num_datasets):\n",
    "    data, ground_truth = generate_dataset(n, nan_ratio)\n",
    "    result = feature_selection_lpcmci(data, ground_truth)\n",
    "    elapsed_time.append(result[1])\n",
    "    f1.append(result[2])\n",
    "    cmi.append(result[3])\n",
    "    recall.append(result[4])\n",
    "\n",
    "# Create DataFrame\n",
    "df = pd.DataFrame({\n",
    "    'Elapsed Time (s)': elapsed_time,\n",
    "    'F1 Score': f1,\n",
    "    'CMI': cmi,\n",
    "    'Recall': recall\n",
    "})\n",
    "\n",
    "# Print the DataFrame\n",
    "print(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'graph': array([[['', '-->'],\n",
      "        ['', ''],\n",
      "        ['', ''],\n",
      "        ['', ''],\n",
      "        ['', '']],\n",
      "\n",
      "       [['', ''],\n",
      "        ['', '-->'],\n",
      "        ['o-o', ''],\n",
      "        ['', ''],\n",
      "        ['', '-->']],\n",
      "\n",
      "       [['', ''],\n",
      "        ['o-o', ''],\n",
      "        ['', '-->'],\n",
      "        ['', ''],\n",
      "        ['-->', '']],\n",
      "\n",
      "       [['', ''],\n",
      "        ['', ''],\n",
      "        ['', ''],\n",
      "        ['', ''],\n",
      "        ['', '']],\n",
      "\n",
      "       [['', '-->'],\n",
      "        ['', ''],\n",
      "        ['<--', ''],\n",
      "        ['', ''],\n",
      "        ['', '']]], dtype='<U3'), 'p_matrix': array([[[0.   ,  -inf],\n",
      "        [0.77 ,   inf],\n",
      "        [0.445, 0.515],\n",
      "        [0.705, 0.305],\n",
      "        [  inf,   inf]],\n",
      "\n",
      "       [[0.77 ,   inf],\n",
      "        [0.   ,  -inf],\n",
      "        [0.   ,   inf],\n",
      "        [0.525,   inf],\n",
      "        [  inf,  -inf]],\n",
      "\n",
      "       [[0.445,   inf],\n",
      "        [0.   ,   inf],\n",
      "        [0.   , 0.   ],\n",
      "        [0.485, 0.51 ],\n",
      "        [ -inf,   inf]],\n",
      "\n",
      "       [[0.705,   inf],\n",
      "        [0.525,   inf],\n",
      "        [0.485,   inf],\n",
      "        [0.   , 0.945],\n",
      "        [  inf,   inf]],\n",
      "\n",
      "       [[  inf,  -inf],\n",
      "        [  inf,   inf],\n",
      "        [ -inf,   inf],\n",
      "        [  inf, 0.53 ],\n",
      "        [0.   ,   inf]]]), 'val_matrix': array([[[0.        ,        inf],\n",
      "        [0.01625765,       -inf],\n",
      "        [0.0176906 , 0.01962324],\n",
      "        [0.01137504, 0.02018355],\n",
      "        [      -inf,       -inf]],\n",
      "\n",
      "       [[0.01625765,       -inf],\n",
      "        [0.        ,        inf],\n",
      "        [0.06316362,       -inf],\n",
      "        [0.0168056 ,       -inf],\n",
      "        [      -inf,        inf]],\n",
      "\n",
      "       [[0.0176906 ,       -inf],\n",
      "        [0.06316362,       -inf],\n",
      "        [0.        , 0.06759722],\n",
      "        [0.02048177, 0.01641789],\n",
      "        [       inf,       -inf]],\n",
      "\n",
      "       [[0.01137504,       -inf],\n",
      "        [0.0168056 ,       -inf],\n",
      "        [0.02048177,       -inf],\n",
      "        [0.        , 0.01398907],\n",
      "        [      -inf,       -inf]],\n",
      "\n",
      "       [[      -inf,        inf],\n",
      "        [      -inf,       -inf],\n",
      "        [       inf,       -inf],\n",
      "        [      -inf, 0.01507007],\n",
      "        [0.        ,       -inf]]])}\n"
     ]
    }
   ],
   "source": [
    "print(result[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'link_assumptions' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[7], line 27\u001b[0m\n\u001b[1;32m     22\u001b[0m cmi_knn \u001b[38;5;241m=\u001b[39m CMIknn(significance\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mshuffle_test\u001b[39m\u001b[38;5;124m'\u001b[39m, knn\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.1\u001b[39m, shuffle_neighbors\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m5\u001b[39m, transform\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mranks\u001b[39m\u001b[38;5;124m'\u001b[39m, sig_samples\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m200\u001b[39m)\n\u001b[1;32m     23\u001b[0m lpcmci_loc \u001b[38;5;241m=\u001b[39m LPCMCI(\n\u001b[1;32m     24\u001b[0m     dataframe\u001b[38;5;241m=\u001b[39mdata, \n\u001b[1;32m     25\u001b[0m     cond_ind_test\u001b[38;5;241m=\u001b[39mcmi_knn,\n\u001b[1;32m     26\u001b[0m     verbosity\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m)\n\u001b[0;32m---> 27\u001b[0m results \u001b[38;5;241m=\u001b[39m lpcmci_loc\u001b[38;5;241m.\u001b[39mrun_lpcmci(tau_max\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m, pc_alpha\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m.2\u001b[39m, link_assumptions \u001b[38;5;241m=\u001b[39m \u001b[43mlink_assumptions\u001b[49m, n_preliminary_iterations \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m)\n\u001b[1;32m     28\u001b[0m tp\u001b[38;5;241m.\u001b[39mplot_time_series_graph(graph\u001b[38;5;241m=\u001b[39mresults[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mgraph\u001b[39m\u001b[38;5;124m'\u001b[39m],\n\u001b[1;32m     29\u001b[0m                           val_matrix\u001b[38;5;241m=\u001b[39mresults[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mval_matrix\u001b[39m\u001b[38;5;124m'\u001b[39m], save_name \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mparcorr_simulated_nolinkass.png\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'link_assumptions' is not defined"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from statsmodels.tsa.stattools import adfuller,kpss\n",
    "import pandas as pd\n",
    "from matplotlib import pyplot as plt\n",
    "import torch\n",
    "import tigramite\n",
    "from tigramite import data_processing as pp\n",
    "from tigramite.toymodels import structural_causal_processes as toys\n",
    "from tigramite import plotting as tp\n",
    "from tigramite.lpcmci import LPCMCI\n",
    "from tigramite.pcmci import PCMCI\n",
    "from tigramite.independence_tests.parcorr import ParCorr\n",
    "from statsmodels.tools.sm_exceptions import InterpolationWarning\n",
    "from tigramite.independence_tests.cmiknn import CMIknn\n",
    "from tigramite.lpcmci import LPCMCI\n",
    "import seaborn as sns\n",
    "#cmi_knn = CMIknn(significance='fixed_thres', model_selection_folds=3)\n",
    "n_a_n = np.isnan(data).any(axis=1)\n",
    "data[n_a_n] = 999\n",
    "data = data.values\n",
    "data = pp.DataFrame(data, var_names = [\"y0\", \"y1\", \"y2\", \"y3\"], missing_flag = 999)\n",
    "cmi_knn = CMIknn(significance='shuffle_test', knn=0.1, shuffle_neighbors=5, transform='ranks', sig_samples=200)\n",
    "lpcmci_loc = LPCMCI(\n",
    "    dataframe=data, \n",
    "    cond_ind_test=cmi_knn,\n",
    "    verbosity=1)\n",
    "results = lpcmci_loc.run_lpcmci(tau_max=1, pc_alpha=.2, link_assumptions = link_assumptions, n_preliminary_iterations = 0)\n",
    "tp.plot_time_series_graph(graph=results['graph'],\n",
    "                          val_matrix=results['val_matrix'], save_name = \"parcorr_simulated_nolinkass.png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=======================================================\n",
      "=======================================================\n",
      "Starting preliminary phase  1\n",
      "\n",
      "Starting test phase\n",
      "\n",
      "p = 0\n",
      "(3,-1) independent (3, 0) given () union set()\n",
      "Writing:   (3,-1) oL> (3, 0) ==> (3,-1)     (3, 0) \n",
      "(0, 0) independent (1, 0) given () union set()\n",
      "(0, 0) independent (1, 0) given () union set()\n",
      "(0, 0) independent (2, 0) given () union set()\n",
      "(0, 0) independent (2, 0) given () union set()\n",
      "(0, 0) independent (3, 0) given () union set()\n",
      "(0, 0) independent (3, 0) given () union set()\n",
      "Writing:   (0, 0) o?o (1, 0) ==> (0, 0)     (1, 0) \n",
      "Writing:   (1, 0) o?o (0, 0) ==> (1, 0)     (0, 0) \n",
      "Writing:   (0, 0) o?o (2, 0) ==> (0, 0)     (2, 0) \n",
      "Writing:   (2, 0) o?o (0, 0) ==> (2, 0)     (0, 0) \n",
      "Writing:   (0, 0) o?o (3, 0) ==> (0, 0)     (3, 0) \n",
      "Writing:   (3, 0) o?o (0, 0) ==> (3, 0)     (0, 0) \n",
      "(0,-1) independent (1, 0) given () union set()\n",
      "(0,-1) independent (3, 0) given () union set()\n",
      "(1,-1) independent (0, 0) given () union set()\n",
      "(1,-1) independent (3, 0) given () union set()\n",
      "(2,-1) independent (0, 0) given () union set()\n",
      "(3,-1) independent (0, 0) given () union set()\n",
      "Writing:   (1,-1) oL> (0, 0) ==> (1,-1)     (0, 0) \n",
      "Writing:   (2,-1) oL> (0, 0) ==> (2,-1)     (0, 0) \n",
      "Writing:   (3,-1) oL> (0, 0) ==> (3,-1)     (0, 0) \n",
      "Writing:   (0,-1) oL> (1, 0) ==> (0,-1)     (1, 0) \n",
      "Writing:   (0,-1) oL> (3, 0) ==> (0,-1)     (3, 0) \n",
      "Writing:   (1,-1) oL> (3, 0) ==> (1,-1)     (3, 0) \n",
      "\n",
      "Test phase complete\n",
      "\n",
      "Starting orientation phase\n",
      "with rule list:  [['APR'], ['ER-08'], ['ER-02'], ['ER-01'], ['ER-09'], ['ER-10']]\n",
      "\n",
      "APR:\n",
      "Found nothing\n",
      "\n",
      "ER-08:\n",
      "Found nothing\n",
      "\n",
      "ER-02:\n",
      "Found nothing\n",
      "\n",
      "ER-01:\n",
      "Found nothing\n",
      "\n",
      "ER-09:\n",
      "Found nothing\n",
      "\n",
      "ER-10:\n",
      "Found nothing\n",
      "\n",
      "Orientation phase complete\n",
      "p = 1\n",
      "Writing:   (0,-1) oL> (0, 0) ==> (0,-1) o!> (0, 0) \n",
      "(1, 0) independent (3, 0) given ((2, 0),) union set()\n",
      "(1, 0) independent (3, 0) given ((2, 0),) union set()\n",
      "Writing:   (1, 0) o?o (3, 0) ==> (1, 0)     (3, 0) \n",
      "Writing:   (3, 0) o?o (1, 0) ==> (3, 0)     (1, 0) \n",
      "(0,-1) independent (2, 0) given ((3, 0),) union set()\n",
      "(1,-1) independent (2, 0) given ((1, 0),) union set()\n",
      "(2,-1) independent (3, 0) given ((2, 0),) union set()\n",
      "(3,-1) independent (1, 0) given ((2, 0),) union set()\n",
      "(3,-1) independent (2, 0) given ((1, 0),) union set()\n",
      "Writing:   (3,-1) oL> (1, 0) ==> (3,-1)     (1, 0) \n",
      "Writing:   (0,-1) oL> (2, 0) ==> (0,-1)     (2, 0) \n",
      "Writing:   (1,-1) oL> (2, 0) ==> (1,-1)     (2, 0) \n",
      "Writing:   (3,-1) oL> (2, 0) ==> (3,-1)     (2, 0) \n",
      "Writing:   (2,-1) oL> (3, 0) ==> (2,-1)     (3, 0) \n",
      "\n",
      "Test phase complete\n",
      "\n",
      "Starting orientation phase\n",
      "with rule list:  [['APR'], ['ER-08'], ['ER-02'], ['ER-01'], ['ER-09'], ['ER-10']]\n",
      "\n",
      "APR:\n",
      "Found nothing\n",
      "\n",
      "ER-08:\n",
      "Found nothing\n",
      "\n",
      "ER-02:\n",
      "Found nothing\n",
      "\n",
      "ER-01:\n",
      "Found nothing\n",
      "\n",
      "ER-09:\n",
      "Found nothing\n",
      "\n",
      "ER-10:\n",
      "Found nothing\n",
      "\n",
      "Orientation phase complete\n",
      "p = 2\n",
      "Writing:   (2, 0) o?o (3, 0) ==> (2, 0) oRo (3, 0) \n",
      "Writing:   (3, 0) o?o (2, 0) ==> (3, 0) oRo (2, 0) \n",
      "\n",
      "Test phase complete\n",
      "p = 3\n",
      "Writing:   (1,-1) oL> (1, 0) ==> (1,-1) o!> (1, 0) \n",
      "Writing:   (2,-1) oL> (2, 0) ==> (2,-1) o!> (2, 0) \n",
      "Writing:   (1, 0) o?o (2, 0) ==> (1, 0) oRo (2, 0) \n",
      "Writing:   (2, 0) o?o (1, 0) ==> (2, 0) oRo (1, 0) \n",
      "Writing:   (1, 0) oRo (2, 0) ==> (1, 0) o!o (2, 0) \n",
      "Writing:   (2, 0) oRo (1, 0) ==> (2, 0) o!o (1, 0) \n",
      "Writing:   (2, 0) oRo (3, 0) ==> (2, 0) o!o (3, 0) \n",
      "Writing:   (3, 0) oRo (2, 0) ==> (3, 0) o!o (2, 0) \n",
      "Writing:   (2,-1) oL> (1, 0) ==> (2,-1) o!> (1, 0) \n",
      "\n",
      "Test phase complete\n",
      "\n",
      "Starting orientation phase\n",
      "with rule list:  [['APR'], ['ER-08'], ['ER-02'], ['ER-01'], ['ER-00-d'], ['ER-00-c'], ['ER-03'], ['R-04'], ['ER-09'], ['ER-10'], ['ER-00-b'], ['ER-00-a']]\n",
      "\n",
      "APR:\n",
      "Found nothing\n",
      "\n",
      "ER-08:\n",
      "Found nothing\n",
      "\n",
      "ER-02:\n",
      "Found nothing\n",
      "\n",
      "ER-01:\n",
      "Marked:    (1, 0) o!o (2, 0) ==> (1, 0) -!> (2, 0) \n",
      "Marked:    (2, 0) o!o (3, 0) ==> (2, 0) -!> (3, 0) \n",
      "Writing:   (2, 0) o!o (1, 0) ==> (2, 0) <!o (1, 0) \n",
      "Writing:   (1, 0) o!o (2, 0) ==> (1, 0) o!> (2, 0) \n",
      "Update:    Marking (2,  0) as non-anc of (1, 0)\n",
      "Writing:   (3, 0) o!o (2, 0) ==> (3, 0) <!o (2, 0) \n",
      "Writing:   (2, 0) o!o (3, 0) ==> (2, 0) o!> (3, 0) \n",
      "Update:    Marking (3,  0) as non-anc of (2, 0)\n",
      "Writing:   (1, 0) o!> (2, 0) ==> (1, 0) -!> (2, 0) \n",
      "Writing:   (2, 0) <!o (1, 0) ==> (2, 0) <!- (1, 0) \n",
      "Update:    Marking (1,  0) as anc of (2, 0)\n",
      "Writing:   (2, 0) o!> (3, 0) ==> (2, 0) -!> (3, 0) \n",
      "Writing:   (3, 0) <!o (2, 0) ==> (3, 0) <!- (2, 0) \n",
      "Update:    Marking (2,  0) as anc of (3, 0)\n",
      "\n",
      "APR:\n",
      "Marked:    (2, 0) -!> (3, 0) ==> (2, 0) --> (3, 0) \n",
      "Marked:    (1, 0) -!> (2, 0) ==> (1, 0) --> (2, 0) \n",
      "Writing:   (1, 0) -!> (2, 0) ==> (1, 0) --> (2, 0) \n",
      "Writing:   (2, 0) <!- (1, 0) ==> (2, 0) <-- (1, 0) \n",
      "Writing:   (2, 0) -!> (3, 0) ==> (2, 0) --> (3, 0) \n",
      "Writing:   (3, 0) <!- (2, 0) ==> (3, 0) <-- (2, 0) \n",
      "\n",
      "APR:\n",
      "Found nothing\n",
      "\n",
      "ER-08:\n",
      "Found nothing\n",
      "\n",
      "ER-02:\n",
      "Found nothing\n",
      "\n",
      "ER-01:\n",
      "Found nothing\n",
      "\n",
      "ER-00-d:\n",
      "Found nothing\n",
      "\n",
      "ER-00-c:\n",
      "Marked:    (2,-1) o!> (1, 0) ==> (2,-1) <!> (1, 0) \n",
      "Writing:   (2,-1) o!> (1, 0) ==> (2,-1) <!> (1, 0) \n",
      "Update:    Marking (2, -1) as non-anc of (1, 0)\n",
      "\n",
      "APR:\n",
      "Found nothing\n",
      "\n",
      "ER-08:\n",
      "Found nothing\n",
      "\n",
      "ER-02:\n",
      "Found nothing\n",
      "\n",
      "ER-01:\n",
      "Found nothing\n",
      "\n",
      "ER-00-d:\n",
      "Found nothing\n",
      "\n",
      "ER-00-c:\n",
      "Found nothing\n",
      "\n",
      "ER-03:\n",
      "Found nothing\n",
      "\n",
      "R-04:\n",
      "Found nothing\n",
      "\n",
      "ER-09:\n",
      "Found nothing\n",
      "\n",
      "ER-10:\n",
      "Found nothing\n",
      "\n",
      "ER-00-b:\n",
      "Found nothing\n",
      "\n",
      "ER-00-a:\n",
      "Found nothing\n",
      "\n",
      "Orientation phase complete\n",
      "\n",
      "Middle mark updates\n",
      "\n",
      "\n",
      "Preliminary phase  1 complete\n",
      "\n",
      "Graph:\n",
      "--------------------------------\n",
      "(0,-1) o!> (0, 0)\n",
      "(1,-1) o!> (1, 0)\n",
      "(2,-1) <!> (1, 0)\n",
      "(1, 0) --> (2, 0)\n",
      "(2,-1) o!> (2, 0)\n",
      "(2, 0) --> (3, 0)\n",
      "--------------------------------\n",
      "Writing:   (2, 0) o?o (1, 0) ==> (2, 0) <?o (1, 0) \n",
      "Writing:   (1, 0) o?o (2, 0) ==> (1, 0) o?> (2, 0) \n",
      "Update:    Marking (2,  0) as non-anc of (1, 0)\n",
      "Writing:   (3, 0) o?o (2, 0) ==> (3, 0) <?o (2, 0) \n",
      "Writing:   (2, 0) o?o (3, 0) ==> (2, 0) o?> (3, 0) \n",
      "Update:    Marking (3,  0) as non-anc of (2, 0)\n",
      "Writing:   (1, 0) o?> (2, 0) ==> (1, 0) -?> (2, 0) \n",
      "Writing:   (2, 0) <?o (1, 0) ==> (2, 0) <?- (1, 0) \n",
      "Update:    Marking (1,  0) as anc of (2, 0)\n",
      "Writing:   (2, 0) o?> (3, 0) ==> (2, 0) -?> (3, 0) \n",
      "Writing:   (3, 0) <?o (2, 0) ==> (3, 0) <?- (2, 0) \n",
      "Update:    Marking (2,  0) as anc of (3, 0)\n",
      "\n",
      "=======================================================\n",
      "=======================================================\n",
      "Starting final ancestral phase\n",
      "\n",
      "Starting test phase\n",
      "\n",
      "p = 0\n",
      "(3,-1) independent (3, 0) given () union {(2, -1), (2, 0)}\n",
      "Writing:   (3,-1) oL> (3, 0) ==> (3,-1)     (3, 0) \n",
      "(0, 0) independent (1, 0) given () union set()\n",
      "(0, 0) independent (1, 0) given () union set()\n",
      "(0, 0) independent (2, 0) given () union {(1, 0)}\n",
      "(0, 0) independent (2, 0) given () union {(1, 0)}\n",
      "(1, 0) independent (3, 0) given () union {(2, 0)}\n",
      "(1, 0) independent (3, 0) given () union {(2, 0)}\n",
      "Writing:   (0, 0) o?o (1, 0) ==> (0, 0)     (1, 0) \n",
      "Writing:   (1, 0) o?o (0, 0) ==> (1, 0)     (0, 0) \n",
      "Writing:   (0, 0) o?o (2, 0) ==> (0, 0)     (2, 0) \n",
      "Writing:   (2, 0) o?o (0, 0) ==> (2, 0)     (0, 0) \n",
      "Writing:   (1, 0) o?o (3, 0) ==> (1, 0)     (3, 0) \n",
      "Writing:   (3, 0) o?o (1, 0) ==> (3, 0)     (1, 0) \n",
      "(0,-1) independent (1, 0) given () union set()\n",
      "(1,-1) independent (0, 0) given () union set()\n",
      "(1,-1) independent (2, 0) given () union {(1, 0)}\n",
      "(1,-1) independent (3, 0) given () union {(2, 0)}\n",
      "(2,-1) independent (0, 0) given () union {(1, -1)}\n",
      "(2,-1) independent (3, 0) given () union {(2, 0), (1, -1)}\n",
      "(3,-1) independent (0, 0) given () union {(2, -1)}\n",
      "(3,-1) independent (2, 0) given () union {(1, 0), (2, -1)}\n",
      "Writing:   (1,-1) oL> (0, 0) ==> (1,-1)     (0, 0) \n",
      "Writing:   (2,-1) oL> (0, 0) ==> (2,-1)     (0, 0) \n",
      "Writing:   (3,-1) oL> (0, 0) ==> (3,-1)     (0, 0) \n",
      "Writing:   (0,-1) oL> (1, 0) ==> (0,-1)     (1, 0) \n",
      "Writing:   (1,-1) oL> (2, 0) ==> (1,-1)     (2, 0) \n",
      "Writing:   (3,-1) oL> (2, 0) ==> (3,-1)     (2, 0) \n",
      "Writing:   (1,-1) oL> (3, 0) ==> (1,-1)     (3, 0) \n",
      "Writing:   (2,-1) oL> (3, 0) ==> (2,-1)     (3, 0) \n",
      "\n",
      "Test phase complete\n",
      "\n",
      "Starting orientation phase\n",
      "with rule list:  [['APR'], ['ER-08'], ['ER-02'], ['ER-01'], ['ER-09'], ['ER-10']]\n",
      "\n",
      "APR:\n",
      "Found nothing\n",
      "\n",
      "ER-08:\n",
      "Found nothing\n",
      "\n",
      "ER-02:\n",
      "Found nothing\n",
      "\n",
      "ER-01:\n",
      "Found nothing\n",
      "\n",
      "ER-09:\n",
      "Found nothing\n",
      "\n",
      "ER-10:\n",
      "Found nothing\n",
      "\n",
      "Orientation phase complete\n",
      "p = 1\n",
      "(0, 0) independent (3, 0) given ((0, -1),) union {(2, 0)}\n",
      "(0, 0) independent (3, 0) given ((0, -1),) union {(2, 0)}\n",
      "Writing:   (0, 0) o?o (3, 0) ==> (0, 0)     (3, 0) \n",
      "Writing:   (3, 0) o?o (0, 0) ==> (3, 0)     (0, 0) \n",
      "(0,-1) independent (2, 0) given ((2, -1),) union {(1, 0)}\n",
      "Writing:   (0,-1) oL> (3, 0) ==> (0,-1) o!> (3, 0) \n",
      "Writing:   (0,-1) oL> (2, 0) ==> (0,-1)     (2, 0) \n",
      "\n",
      "Test phase complete\n",
      "\n",
      "Starting orientation phase\n",
      "with rule list:  [['APR'], ['ER-08'], ['ER-02'], ['ER-01'], ['ER-09'], ['ER-10']]\n",
      "\n",
      "APR:\n",
      "Found nothing\n",
      "\n",
      "ER-08:\n",
      "Found nothing\n",
      "\n",
      "ER-02:\n",
      "Found nothing\n",
      "\n",
      "ER-01:\n",
      "Found nothing\n",
      "\n",
      "ER-09:\n",
      "Found nothing\n",
      "\n",
      "ER-10:\n",
      "Found nothing\n",
      "\n",
      "Orientation phase complete\n",
      "p = 2\n",
      "Writing:   (0,-1) oL> (0, 0) ==> (0,-1) o!> (0, 0) \n",
      "Writing:   (2,-1) oL> (2, 0) ==> (2,-1) o!> (2, 0) \n",
      "Writing:   (1, 0) -?> (2, 0) ==> (1, 0) -R> (2, 0) \n",
      "Writing:   (2, 0) <?- (1, 0) ==> (2, 0) <R- (1, 0) \n",
      "Writing:   (2, 0) -?> (3, 0) ==> (2, 0) -R> (3, 0) \n",
      "Writing:   (3, 0) <?- (2, 0) ==> (3, 0) <R- (2, 0) \n",
      "Writing:   (2, 0) -R> (3, 0) ==> (2, 0) -!> (3, 0) \n",
      "Writing:   (3, 0) <R- (2, 0) ==> (3, 0) <!- (2, 0) \n",
      "Writing:   (2,-1) oL> (1, 0) ==> (2,-1) o!> (1, 0) \n",
      "Writing:   (3,-1) oL> (1, 0) ==> (3,-1) o!> (1, 0) \n",
      "\n",
      "Test phase complete\n",
      "p = 3\n",
      "Writing:   (1,-1) oL> (1, 0) ==> (1,-1) o!> (1, 0) \n",
      "\n",
      "Test phase complete\n",
      "p = 4\n",
      "Writing:   (1, 0) -R> (2, 0) ==> (1, 0) -!> (2, 0) \n",
      "Writing:   (2, 0) <R- (1, 0) ==> (2, 0) <!- (1, 0) \n",
      "\n",
      "Test phase complete\n",
      "\n",
      "Starting orientation phase\n",
      "with rule list:  [['APR'], ['ER-08'], ['ER-02'], ['ER-01'], ['ER-00-d'], ['ER-00-c'], ['ER-03'], ['R-04'], ['ER-09'], ['ER-10'], ['ER-00-b'], ['ER-00-a']]\n",
      "\n",
      "APR:\n",
      "Marked:    (2, 0) -!> (3, 0) ==> (2, 0) --> (3, 0) \n",
      "Marked:    (1, 0) -!> (2, 0) ==> (1, 0) --> (2, 0) \n",
      "Writing:   (1, 0) -!> (2, 0) ==> (1, 0) --> (2, 0) \n",
      "Writing:   (2, 0) <!- (1, 0) ==> (2, 0) <-- (1, 0) \n",
      "Writing:   (2, 0) -!> (3, 0) ==> (2, 0) --> (3, 0) \n",
      "Writing:   (3, 0) <!- (2, 0) ==> (3, 0) <-- (2, 0) \n",
      "\n",
      "APR:\n",
      "Found nothing\n",
      "\n",
      "ER-08:\n",
      "Found nothing\n",
      "\n",
      "ER-02:\n",
      "Found nothing\n",
      "\n",
      "ER-01:\n",
      "Found nothing\n",
      "\n",
      "ER-00-d:\n",
      "Found nothing\n",
      "\n",
      "ER-00-c:\n",
      "Found nothing\n",
      "\n",
      "ER-03:\n",
      "Found nothing\n",
      "\n",
      "R-04:\n",
      "Found nothing\n",
      "\n",
      "ER-09:\n",
      "Found nothing\n",
      "\n",
      "ER-10:\n",
      "Found nothing\n",
      "\n",
      "ER-00-b:\n",
      "Found nothing\n",
      "\n",
      "ER-00-a:\n",
      "Found nothing\n",
      "\n",
      "Orientation phase complete\n",
      "\n",
      "Middle mark updates\n",
      "\n",
      "\n",
      "Final ancestral phase complete\n",
      "\n",
      "Graph:\n",
      "--------------------------------\n",
      "(0,-1) o!> (0, 0)\n",
      "(1,-1) o!> (1, 0)\n",
      "(2,-1) o!> (1, 0)\n",
      "(3,-1) o!> (1, 0)\n",
      "(1, 0) --> (2, 0)\n",
      "(2,-1) o!> (2, 0)\n",
      "(2, 0) --> (3, 0)\n",
      "(0,-1) o!> (3, 0)\n",
      "--------------------------------\n",
      "\n",
      "=======================================================\n",
      "=======================================================\n",
      "Starting non-ancestral phase\n",
      "\n",
      "Middle mark updates\n",
      "\n",
      "\n",
      "Starting test phase\n",
      "\n",
      "p = 0\n",
      "\n",
      "Test phase complete\n",
      "p = 1\n",
      "Writing:   (0,-1) o!> (0, 0) ==> (0,-1) o-> (0, 0) \n",
      "Writing:   (2,-1) o!> (2, 0) ==> (2,-1) o-> (2, 0) \n",
      "Writing:   (0,-1) o!> (3, 0) ==> (0,-1) o-> (3, 0) \n",
      "Writing:   (2,-1) o!> (1, 0) ==> (2,-1) o-> (1, 0) \n",
      "\n",
      "Test phase complete\n",
      "p = 2\n",
      "Writing:   (1,-1) o!> (1, 0) ==> (1,-1) o-> (1, 0) \n",
      "Writing:   (3,-1) o!> (1, 0) ==> (3,-1) o-> (1, 0) \n",
      "\n",
      "Test phase complete\n",
      "\n",
      "Starting orientation phase\n",
      "with rule list:  [['APR'], ['ER-08'], ['ER-02'], ['ER-01'], ['ER-00-d'], ['ER-00-c'], ['ER-03'], ['R-04'], ['ER-09'], ['ER-10'], ['ER-00-b'], ['ER-00-a']]\n",
      "\n",
      "APR:\n",
      "Found nothing\n",
      "\n",
      "ER-08:\n",
      "Found nothing\n",
      "\n",
      "ER-02:\n",
      "Found nothing\n",
      "\n",
      "ER-01:\n",
      "Found nothing\n",
      "\n",
      "ER-00-d:\n",
      "Found nothing\n",
      "\n",
      "ER-00-c:\n",
      "Found nothing\n",
      "\n",
      "ER-03:\n",
      "Found nothing\n",
      "\n",
      "R-04:\n",
      "Marked:    (2,-1) o-> (2, 0) ==> (2,-1) --> (2, 0) \n",
      "Writing:   (2,-1) o-> (2, 0) ==> (2,-1) --> (2, 0) \n",
      "Update:    Marking (2, -1) as anc of (2, 0)\n",
      "\n",
      "APR:\n",
      "Found nothing\n",
      "\n",
      "ER-08:\n",
      "Found nothing\n",
      "\n",
      "ER-02:\n",
      "Found nothing\n",
      "\n",
      "ER-01:\n",
      "Found nothing\n",
      "\n",
      "ER-00-d:\n",
      "Found nothing\n",
      "\n",
      "ER-00-c:\n",
      "Found nothing\n",
      "\n",
      "ER-03:\n",
      "Found nothing\n",
      "\n",
      "R-04:\n",
      "Found nothing\n",
      "\n",
      "ER-09:\n",
      "Found nothing\n",
      "\n",
      "ER-10:\n",
      "Found nothing\n",
      "\n",
      "ER-00-b:\n",
      "Found nothing\n",
      "\n",
      "ER-00-a:\n",
      "Found nothing\n",
      "\n",
      "Orientation phase complete\n",
      "\n",
      "Non-ancestral phase complete\n",
      "\n",
      "Graph:\n",
      "--------------------------------\n",
      "(0,-1) o-> (0, 0)\n",
      "(1,-1) o-> (1, 0)\n",
      "(2,-1) o-> (1, 0)\n",
      "(3,-1) o-> (1, 0)\n",
      "(1, 0) --> (2, 0)\n",
      "(2,-1) --> (2, 0)\n",
      "(2, 0) --> (3, 0)\n",
      "(0,-1) o-> (3, 0)\n",
      "--------------------------------\n",
      "\n",
      "=======================================================\n",
      "=======================================================\n",
      "Final rule application phase\n",
      "\n",
      "Setting all middle marks to '-'\n",
      "\n",
      "Starting orientation phase\n",
      "with rule list:  [['APR'], ['ER-08'], ['ER-02'], ['ER-01'], ['ER-00-d'], ['ER-00-c'], ['ER-03'], ['R-04'], ['ER-09'], ['ER-10'], ['ER-00-b'], ['ER-00-a']]\n",
      "\n",
      "APR:\n",
      "Found nothing\n",
      "\n",
      "ER-08:\n",
      "Found nothing\n",
      "\n",
      "ER-02:\n",
      "Found nothing\n",
      "\n",
      "ER-01:\n",
      "Found nothing\n",
      "\n",
      "ER-00-d:\n",
      "Found nothing\n",
      "\n",
      "ER-00-c:\n",
      "Found nothing\n",
      "\n",
      "ER-03:\n",
      "Found nothing\n",
      "\n",
      "R-04:\n",
      "Found nothing\n",
      "\n",
      "ER-09:\n",
      "Found nothing\n",
      "\n",
      "ER-10:\n",
      "Found nothing\n",
      "\n",
      "ER-00-b:\n",
      "Found nothing\n",
      "\n",
      "ER-00-a:\n",
      "Found nothing\n",
      "\n",
      "Orientation phase complete\n",
      "\n",
      "=======================================================\n",
      "=======================================================\n",
      "\n",
      "LPCMCI has converged\n",
      "\n",
      "Final graph:\n",
      "--------------------------------\n",
      "--------------------------------\n",
      "(0,-1) o-> (0, 0)\n",
      "(1,-1) o-> (1, 0)\n",
      "(2,-1) o-> (1, 0)\n",
      "(3,-1) o-> (1, 0)\n",
      "(1, 0) --> (2, 0)\n",
      "(2,-1) --> (2, 0)\n",
      "(2, 0) --> (3, 0)\n",
      "(0,-1) o-> (3, 0)\n",
      "--------------------------------\n",
      "--------------------------------\n",
      "\n",
      "Max search set: 1\n",
      "Max na-pds set: 2\n",
      "\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "cmi_knn = CMIknn(significance='shuffle_test', knn=0.1, shuffle_neighbors=5, transform='ranks', sig_samples=200)\n",
    "lpcmci_loc = LPCMCI(\n",
    "    dataframe=data, \n",
    "    cond_ind_test=cmi_knn,\n",
    "    verbosity=1)\n",
    "results = lpcmci_loc.run_lpcmci(tau_max=1, pc_alpha=.2, n_preliminary_iterations = 1)\n",
    "tp.plot_time_series_graph(graph=results['graph'],\n",
    "                          val_matrix=results['val_matrix'], save_name = \"parcorr_simulated_nolinkass.png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=======================================================\n",
      "=======================================================\n",
      "Starting preliminary phase  1\n",
      "\n",
      "Starting test phase\n",
      "\n",
      "p = 0\n",
      "(3,-1) independent (3, 0) given () union set()\n",
      "Writing:   (3,-1) oL> (3, 0) ==> (3,-1)     (3, 0) \n",
      "(0, 0) independent (1, 0) given () union set()\n",
      "(0, 0) independent (1, 0) given () union set()\n"
     ]
    }
   ],
   "source": [
    "cmi_knn = CMIknn(significance='shuffle_test', knn=0.1, shuffle_neighbors=5, transform='ranks', sig_samples=200)\n",
    "lpcmci_loc = LPCMCI(\n",
    "    dataframe=data, \n",
    "    cond_ind_test=cmi_knn,\n",
    "    verbosity=1)\n",
    "results = lpcmci_loc.run_lpcmci(tau_max=1, pc_alpha=.05, n_preliminary_iterations = 1)\n",
    "tp.plot_time_series_graph(graph=results['graph'],\n",
    "                          val_matrix=results['val_matrix'], save_name = \"parcorr_simulated_nolinkass.png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating 0 lags: 100%|██████████| 2/2 [00:00<00:00, 20.76it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      0_lag1\n",
      "1   66.21673\n",
      "2  200.73140\n",
      "3   24.65150\n",
      "4  159.60626\n",
      "5   12.18114\n",
      "1    200.73140\n",
      "2     24.65150\n",
      "3    159.60626\n",
      "4     12.18114\n",
      "5    270.90903\n",
      "Name: 0, dtype: float64\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[8], line 320\u001b[0m\n\u001b[1;32m    318\u001b[0m dat \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mread_csv(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m/home/gnicolaou/tigramite/tutorials/causal_discovery/data.csv\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m    319\u001b[0m dat \u001b[38;5;241m=\u001b[39m dat\u001b[38;5;241m.\u001b[39miloc[\u001b[38;5;241m0\u001b[39m:\u001b[38;5;241m400\u001b[39m]\n\u001b[0;32m--> 320\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[43mfeature_selection_lpcmci\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdat\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    321\u001b[0m \u001b[38;5;28mprint\u001b[39m(result)\n",
      "Cell \u001b[0;32mIn[8], line 229\u001b[0m, in \u001b[0;36mfeature_selection_lpcmci\u001b[0;34m(data, gt, lag, test_size, random_state, alpha, n_shuffles)\u001b[0m\n\u001b[1;32m    227\u001b[0m X_train_shuffled, X_test_shuffled, y_train_shuffled, y_test_shuffled \u001b[38;5;241m=\u001b[39m train_test_split(shuffled_features, target_column, test_size\u001b[38;5;241m=\u001b[39mtest_size, random_state\u001b[38;5;241m=\u001b[39mrandom_state)\n\u001b[1;32m    228\u001b[0m shuffled_model \u001b[38;5;241m=\u001b[39m RandomForestRegressor(random_state\u001b[38;5;241m=\u001b[39mrandom_state)\n\u001b[0;32m--> 229\u001b[0m \u001b[43mshuffled_model\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX_train_shuffled\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_train_shuffled\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    231\u001b[0m shuffled_predictions \u001b[38;5;241m=\u001b[39m shuffled_model\u001b[38;5;241m.\u001b[39mpredict(X_test_shuffled)\n\u001b[1;32m    232\u001b[0m shuffle_mse\u001b[38;5;241m.\u001b[39mappend(mean_squared_error(y_test_shuffled, shuffled_predictions))\n",
      "File \u001b[0;32m~/tigramite/.venv/lib/python3.10/site-packages/sklearn/base.py:1152\u001b[0m, in \u001b[0;36m_fit_context.<locals>.decorator.<locals>.wrapper\u001b[0;34m(estimator, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1145\u001b[0m     estimator\u001b[38;5;241m.\u001b[39m_validate_params()\n\u001b[1;32m   1147\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m config_context(\n\u001b[1;32m   1148\u001b[0m     skip_parameter_validation\u001b[38;5;241m=\u001b[39m(\n\u001b[1;32m   1149\u001b[0m         prefer_skip_nested_validation \u001b[38;5;129;01mor\u001b[39;00m global_skip_validation\n\u001b[1;32m   1150\u001b[0m     )\n\u001b[1;32m   1151\u001b[0m ):\n\u001b[0;32m-> 1152\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfit_method\u001b[49m\u001b[43m(\u001b[49m\u001b[43mestimator\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/tigramite/.venv/lib/python3.10/site-packages/sklearn/ensemble/_forest.py:445\u001b[0m, in \u001b[0;36mBaseForest.fit\u001b[0;34m(self, X, y, sample_weight)\u001b[0m\n\u001b[1;32m    440\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mwarm_start \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mestimators_) \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[1;32m    441\u001b[0m     \u001b[38;5;66;03m# We draw from the random state to get the random state we\u001b[39;00m\n\u001b[1;32m    442\u001b[0m     \u001b[38;5;66;03m# would have got if we hadn't used a warm_start.\u001b[39;00m\n\u001b[1;32m    443\u001b[0m     random_state\u001b[38;5;241m.\u001b[39mrandint(MAX_INT, size\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mestimators_))\n\u001b[0;32m--> 445\u001b[0m trees \u001b[38;5;241m=\u001b[39m [\n\u001b[1;32m    446\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_make_estimator(append\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m, random_state\u001b[38;5;241m=\u001b[39mrandom_state)\n\u001b[1;32m    447\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(n_more_estimators)\n\u001b[1;32m    448\u001b[0m ]\n\u001b[1;32m    450\u001b[0m \u001b[38;5;66;03m# Parallel loop: we prefer the threading backend as the Cython code\u001b[39;00m\n\u001b[1;32m    451\u001b[0m \u001b[38;5;66;03m# for fitting the trees is internally releasing the Python GIL\u001b[39;00m\n\u001b[1;32m    452\u001b[0m \u001b[38;5;66;03m# making threading more efficient than multiprocessing in\u001b[39;00m\n\u001b[1;32m    453\u001b[0m \u001b[38;5;66;03m# that case. However, for joblib 0.12+ we respect any\u001b[39;00m\n\u001b[1;32m    454\u001b[0m \u001b[38;5;66;03m# parallel_backend contexts set at a higher level,\u001b[39;00m\n\u001b[1;32m    455\u001b[0m \u001b[38;5;66;03m# since correctness does not rely on using threads.\u001b[39;00m\n\u001b[1;32m    456\u001b[0m trees \u001b[38;5;241m=\u001b[39m Parallel(\n\u001b[1;32m    457\u001b[0m     n_jobs\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mn_jobs,\n\u001b[1;32m    458\u001b[0m     verbose\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mverbose,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    473\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m i, t \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(trees)\n\u001b[1;32m    474\u001b[0m )\n",
      "File \u001b[0;32m~/tigramite/.venv/lib/python3.10/site-packages/sklearn/ensemble/_forest.py:446\u001b[0m, in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    440\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mwarm_start \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mestimators_) \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[1;32m    441\u001b[0m     \u001b[38;5;66;03m# We draw from the random state to get the random state we\u001b[39;00m\n\u001b[1;32m    442\u001b[0m     \u001b[38;5;66;03m# would have got if we hadn't used a warm_start.\u001b[39;00m\n\u001b[1;32m    443\u001b[0m     random_state\u001b[38;5;241m.\u001b[39mrandint(MAX_INT, size\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mestimators_))\n\u001b[1;32m    445\u001b[0m trees \u001b[38;5;241m=\u001b[39m [\n\u001b[0;32m--> 446\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_make_estimator\u001b[49m\u001b[43m(\u001b[49m\u001b[43mappend\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrandom_state\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrandom_state\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    447\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(n_more_estimators)\n\u001b[1;32m    448\u001b[0m ]\n\u001b[1;32m    450\u001b[0m \u001b[38;5;66;03m# Parallel loop: we prefer the threading backend as the Cython code\u001b[39;00m\n\u001b[1;32m    451\u001b[0m \u001b[38;5;66;03m# for fitting the trees is internally releasing the Python GIL\u001b[39;00m\n\u001b[1;32m    452\u001b[0m \u001b[38;5;66;03m# making threading more efficient than multiprocessing in\u001b[39;00m\n\u001b[1;32m    453\u001b[0m \u001b[38;5;66;03m# that case. However, for joblib 0.12+ we respect any\u001b[39;00m\n\u001b[1;32m    454\u001b[0m \u001b[38;5;66;03m# parallel_backend contexts set at a higher level,\u001b[39;00m\n\u001b[1;32m    455\u001b[0m \u001b[38;5;66;03m# since correctness does not rely on using threads.\u001b[39;00m\n\u001b[1;32m    456\u001b[0m trees \u001b[38;5;241m=\u001b[39m Parallel(\n\u001b[1;32m    457\u001b[0m     n_jobs\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mn_jobs,\n\u001b[1;32m    458\u001b[0m     verbose\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mverbose,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    473\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m i, t \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(trees)\n\u001b[1;32m    474\u001b[0m )\n",
      "File \u001b[0;32m~/tigramite/.venv/lib/python3.10/site-packages/sklearn/ensemble/_base.py:191\u001b[0m, in \u001b[0;36mBaseEnsemble._make_estimator\u001b[0;34m(self, append, random_state)\u001b[0m\n\u001b[1;32m    188\u001b[0m estimator\u001b[38;5;241m.\u001b[39mset_params(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39m{p: \u001b[38;5;28mgetattr\u001b[39m(\u001b[38;5;28mself\u001b[39m, p) \u001b[38;5;28;01mfor\u001b[39;00m p \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mestimator_params})\n\u001b[1;32m    190\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m random_state \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m--> 191\u001b[0m     \u001b[43m_set_random_states\u001b[49m\u001b[43m(\u001b[49m\u001b[43mestimator\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrandom_state\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    193\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m append:\n\u001b[1;32m    194\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mestimators_\u001b[38;5;241m.\u001b[39mappend(estimator)\n",
      "File \u001b[0;32m~/tigramite/.venv/lib/python3.10/site-packages/sklearn/ensemble/_base.py:75\u001b[0m, in \u001b[0;36m_set_random_states\u001b[0;34m(estimator, random_state)\u001b[0m\n\u001b[1;32m     72\u001b[0m         to_set[key] \u001b[38;5;241m=\u001b[39m random_state\u001b[38;5;241m.\u001b[39mrandint(np\u001b[38;5;241m.\u001b[39miinfo(np\u001b[38;5;241m.\u001b[39mint32)\u001b[38;5;241m.\u001b[39mmax)\n\u001b[1;32m     74\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m to_set:\n\u001b[0;32m---> 75\u001b[0m     \u001b[43mestimator\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mset_params\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mto_set\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/tigramite/.venv/lib/python3.10/site-packages/sklearn/base.py:223\u001b[0m, in \u001b[0;36mBaseEstimator.set_params\u001b[0;34m(self, **params)\u001b[0m\n\u001b[1;32m    220\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m params:\n\u001b[1;32m    221\u001b[0m     \u001b[38;5;66;03m# Simple optimization to gain speed (inspect is slow)\u001b[39;00m\n\u001b[1;32m    222\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\n\u001b[0;32m--> 223\u001b[0m valid_params \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_params\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdeep\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[1;32m    225\u001b[0m nested_params \u001b[38;5;241m=\u001b[39m defaultdict(\u001b[38;5;28mdict\u001b[39m)  \u001b[38;5;66;03m# grouped by prefix\u001b[39;00m\n\u001b[1;32m    226\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m key, value \u001b[38;5;129;01min\u001b[39;00m params\u001b[38;5;241m.\u001b[39mitems():\n",
      "File \u001b[0;32m~/tigramite/.venv/lib/python3.10/site-packages/sklearn/base.py:194\u001b[0m, in \u001b[0;36mBaseEstimator.get_params\u001b[0;34m(self, deep)\u001b[0m\n\u001b[1;32m    179\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    180\u001b[0m \u001b[38;5;124;03mGet parameters for this estimator.\u001b[39;00m\n\u001b[1;32m    181\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    191\u001b[0m \u001b[38;5;124;03m    Parameter names mapped to their values.\u001b[39;00m\n\u001b[1;32m    192\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    193\u001b[0m out \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mdict\u001b[39m()\n\u001b[0;32m--> 194\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m key \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_get_param_names\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m:\n\u001b[1;32m    195\u001b[0m     value \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mgetattr\u001b[39m(\u001b[38;5;28mself\u001b[39m, key)\n\u001b[1;32m    196\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m deep \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(value, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mget_params\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(value, \u001b[38;5;28mtype\u001b[39m):\n",
      "File \u001b[0;32m~/tigramite/.venv/lib/python3.10/site-packages/sklearn/base.py:159\u001b[0m, in \u001b[0;36mBaseEstimator._get_param_names\u001b[0;34m(cls)\u001b[0m\n\u001b[1;32m    155\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m []\n\u001b[1;32m    157\u001b[0m \u001b[38;5;66;03m# introspect the constructor arguments to find the model parameters\u001b[39;00m\n\u001b[1;32m    158\u001b[0m \u001b[38;5;66;03m# to represent\u001b[39;00m\n\u001b[0;32m--> 159\u001b[0m init_signature \u001b[38;5;241m=\u001b[39m \u001b[43minspect\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msignature\u001b[49m\u001b[43m(\u001b[49m\u001b[43minit\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    160\u001b[0m \u001b[38;5;66;03m# Consider the constructor parameters excluding 'self'\u001b[39;00m\n\u001b[1;32m    161\u001b[0m parameters \u001b[38;5;241m=\u001b[39m [\n\u001b[1;32m    162\u001b[0m     p\n\u001b[1;32m    163\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m p \u001b[38;5;129;01min\u001b[39;00m init_signature\u001b[38;5;241m.\u001b[39mparameters\u001b[38;5;241m.\u001b[39mvalues()\n\u001b[1;32m    164\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m p\u001b[38;5;241m.\u001b[39mname \u001b[38;5;241m!=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mself\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m p\u001b[38;5;241m.\u001b[39mkind \u001b[38;5;241m!=\u001b[39m p\u001b[38;5;241m.\u001b[39mVAR_KEYWORD\n\u001b[1;32m    165\u001b[0m ]\n",
      "File \u001b[0;32m/usr/lib/python3.10/inspect.py:3247\u001b[0m, in \u001b[0;36msignature\u001b[0;34m(obj, follow_wrapped, globals, locals, eval_str)\u001b[0m\n\u001b[1;32m   3245\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21msignature\u001b[39m(obj, \u001b[38;5;241m*\u001b[39m, follow_wrapped\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m, \u001b[38;5;28mglobals\u001b[39m\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, \u001b[38;5;28mlocals\u001b[39m\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, eval_str\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m):\n\u001b[1;32m   3246\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Get a signature object for the passed callable.\"\"\"\u001b[39;00m\n\u001b[0;32m-> 3247\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mSignature\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfrom_callable\u001b[49m\u001b[43m(\u001b[49m\u001b[43mobj\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfollow_wrapped\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfollow_wrapped\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3248\u001b[0m \u001b[43m                                   \u001b[49m\u001b[38;5;28;43mglobals\u001b[39;49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mglobals\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mlocals\u001b[39;49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mlocals\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43meval_str\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43meval_str\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/usr/lib/python3.10/inspect.py:2995\u001b[0m, in \u001b[0;36mSignature.from_callable\u001b[0;34m(cls, obj, follow_wrapped, globals, locals, eval_str)\u001b[0m\n\u001b[1;32m   2991\u001b[0m \u001b[38;5;129m@classmethod\u001b[39m\n\u001b[1;32m   2992\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mfrom_callable\u001b[39m(\u001b[38;5;28mcls\u001b[39m, obj, \u001b[38;5;241m*\u001b[39m,\n\u001b[1;32m   2993\u001b[0m                   follow_wrapped\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m, \u001b[38;5;28mglobals\u001b[39m\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, \u001b[38;5;28mlocals\u001b[39m\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, eval_str\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m):\n\u001b[1;32m   2994\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Constructs Signature for the given callable object.\"\"\"\u001b[39;00m\n\u001b[0;32m-> 2995\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_signature_from_callable\u001b[49m\u001b[43m(\u001b[49m\u001b[43mobj\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msigcls\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mcls\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2996\u001b[0m \u001b[43m                                    \u001b[49m\u001b[43mfollow_wrapper_chains\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfollow_wrapped\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2997\u001b[0m \u001b[43m                                    \u001b[49m\u001b[38;5;28;43mglobals\u001b[39;49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mglobals\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mlocals\u001b[39;49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mlocals\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43meval_str\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43meval_str\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/usr/lib/python3.10/inspect.py:2456\u001b[0m, in \u001b[0;36m_signature_from_callable\u001b[0;34m(obj, follow_wrapper_chains, skip_bound_arg, globals, locals, eval_str, sigcls)\u001b[0m\n\u001b[1;32m   2451\u001b[0m             \u001b[38;5;28;01mreturn\u001b[39;00m sig\u001b[38;5;241m.\u001b[39mreplace(parameters\u001b[38;5;241m=\u001b[39mnew_params)\n\u001b[1;32m   2453\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m isfunction(obj) \u001b[38;5;129;01mor\u001b[39;00m _signature_is_functionlike(obj):\n\u001b[1;32m   2454\u001b[0m     \u001b[38;5;66;03m# If it's a pure Python function, or an object that is duck type\u001b[39;00m\n\u001b[1;32m   2455\u001b[0m     \u001b[38;5;66;03m# of a Python function (Cython functions, for instance), then:\u001b[39;00m\n\u001b[0;32m-> 2456\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_signature_from_function\u001b[49m\u001b[43m(\u001b[49m\u001b[43msigcls\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mobj\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2457\u001b[0m \u001b[43m                                    \u001b[49m\u001b[43mskip_bound_arg\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mskip_bound_arg\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2458\u001b[0m \u001b[43m                                    \u001b[49m\u001b[38;5;28;43mglobals\u001b[39;49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mglobals\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mlocals\u001b[39;49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mlocals\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43meval_str\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43meval_str\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   2460\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m _signature_is_builtin(obj):\n\u001b[1;32m   2461\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m _signature_from_builtin(sigcls, obj,\n\u001b[1;32m   2462\u001b[0m                                    skip_bound_arg\u001b[38;5;241m=\u001b[39mskip_bound_arg)\n",
      "File \u001b[0;32m/usr/lib/python3.10/inspect.py:2350\u001b[0m, in \u001b[0;36m_signature_from_function\u001b[0;34m(cls, func, skip_bound_arg, globals, locals, eval_str)\u001b[0m\n\u001b[1;32m   2347\u001b[0m         default \u001b[38;5;241m=\u001b[39m kwdefaults\u001b[38;5;241m.\u001b[39mget(name, _empty)\n\u001b[1;32m   2349\u001b[0m     annotation \u001b[38;5;241m=\u001b[39m annotations\u001b[38;5;241m.\u001b[39mget(name, _empty)\n\u001b[0;32m-> 2350\u001b[0m     parameters\u001b[38;5;241m.\u001b[39mappend(\u001b[43mParameter\u001b[49m\u001b[43m(\u001b[49m\u001b[43mname\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mannotation\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mannotation\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2351\u001b[0m \u001b[43m                                \u001b[49m\u001b[43mkind\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m_KEYWORD_ONLY\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2352\u001b[0m \u001b[43m                                \u001b[49m\u001b[43mdefault\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdefault\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[1;32m   2353\u001b[0m \u001b[38;5;66;03m# **kwargs\u001b[39;00m\n\u001b[1;32m   2354\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m func_code\u001b[38;5;241m.\u001b[39mco_flags \u001b[38;5;241m&\u001b[39m CO_VARKEYWORDS:\n",
      "File \u001b[0;32m/usr/lib/python3.10/inspect.py:2632\u001b[0m, in \u001b[0;36mParameter.__init__\u001b[0;34m(self, name, kind, default, annotation)\u001b[0m\n\u001b[1;32m   2630\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__init__\u001b[39m(\u001b[38;5;28mself\u001b[39m, name, kind, \u001b[38;5;241m*\u001b[39m, default\u001b[38;5;241m=\u001b[39m_empty, annotation\u001b[38;5;241m=\u001b[39m_empty):\n\u001b[1;32m   2631\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m-> 2632\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_kind \u001b[38;5;241m=\u001b[39m \u001b[43m_ParameterKind\u001b[49m\u001b[43m(\u001b[49m\u001b[43mkind\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   2633\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m:\n\u001b[1;32m   2634\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mvalue \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mkind\u001b[38;5;132;01m!r}\u001b[39;00m\u001b[38;5;124m is not a valid Parameter.kind\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "File \u001b[0;32m/usr/lib/python3.10/enum.py:359\u001b[0m, in \u001b[0;36mEnumMeta.__call__\u001b[0;34m(cls, value, names, module, qualname, type, start)\u001b[0m\n\u001b[1;32m    354\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    355\u001b[0m \u001b[38;5;124;03m    classes/types should always be True.\u001b[39;00m\n\u001b[1;32m    356\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m    357\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[0;32m--> 359\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__call__\u001b[39m(\u001b[38;5;28mcls\u001b[39m, value, names\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, \u001b[38;5;241m*\u001b[39m, module\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, qualname\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, \u001b[38;5;28mtype\u001b[39m\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, start\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m):\n\u001b[1;32m    360\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    361\u001b[0m \u001b[38;5;124;03m    Either returns an existing member, or creates a new enum class.\u001b[39;00m\n\u001b[1;32m    362\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    382\u001b[0m \u001b[38;5;124;03m    `type`, if set, will be mixed in as the first base class.\u001b[39;00m\n\u001b[1;32m    383\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m    384\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m names \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:  \u001b[38;5;66;03m# simple value lookup\u001b[39;00m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.model_selection import train_test_split\n",
    "from scipy.stats import norm\n",
    "import numpy as np\n",
    "from statsmodels.tsa.stattools import adfuller,kpss\n",
    "import pandas as pd\n",
    "from matplotlib import pyplot as plt\n",
    "from tigramite import data_processing as pp\n",
    "from tigramite.toymodels import structural_causal_processes as toys\n",
    "from tigramite import plotting as tp\n",
    "from tigramite.lpcmci import LPCMCI\n",
    "from tigramite.pcmci import PCMCI\n",
    "from tigramite.independence_tests.parcorr import ParCorr\n",
    "from statsmodels.tools.sm_exceptions import InterpolationWarning\n",
    "from tigramite.independence_tests.cmiknn import CMIknn\n",
    "from tigramite.lpcmci import LPCMCI\n",
    "import seaborn as sns\n",
    "from tqdm import tqdm\n",
    "import time\n",
    "\n",
    "def compute_f1_score(ground_truth, predicted):\n",
    "    # Define equivalences for causal relationships\n",
    "    equivalences = {\n",
    "        '-->': '-?>',\n",
    "        '<--': '<?-',\n",
    "        'o-o': {'<-o', 'o->', '<->'}\n",
    "    }\n",
    "    \n",
    "    true_positives = 0\n",
    "    false_positives = 0\n",
    "    false_negatives = 0\n",
    "    \n",
    "    # Iterate through the arrays and compare relationships\n",
    "    for i in range(ground_truth.shape[0]):\n",
    "        for j in range(ground_truth.shape[1]):\n",
    "            ground_relation = ground_truth[i][j]\n",
    "            predicted_relation = predicted[i][j]\n",
    "            \n",
    "            if ground_relation == '' and predicted_relation == '':\n",
    "                # No relation, skip\n",
    "                continue\n",
    "            elif ground_relation == predicted_relation:\n",
    "                true_positives += 1\n",
    "            elif ground_relation in equivalences:\n",
    "                # Check if predicted relation matches any of the equivalences\n",
    "                if predicted_relation in equivalences[ground_relation] if isinstance(equivalences[ground_relation], set) else predicted_relation == equivalences[ground_relation]:\n",
    "                    true_positives += 1\n",
    "                else:\n",
    "                    false_positives += 1\n",
    "            else:\n",
    "                # If the predicted relation doesn't match the ground truth, it's a false positive\n",
    "                if predicted_relation != '':\n",
    "                    false_positives += 1\n",
    "                # Missed causal link in the predicted array -> false negative\n",
    "                if ground_relation != '':\n",
    "                    false_negatives += 1\n",
    "\n",
    "    # Calculate precision, recall, and F1 score\n",
    "    precision = true_positives / (true_positives + false_positives) if true_positives + false_positives > 0 else 0\n",
    "    recall = true_positives / (true_positives + false_negatives) if true_positives + false_negatives > 0 else 0\n",
    "    f1_score = 2 * precision * recall / (precision + recall) if precision + recall > 0 else 0\n",
    "\n",
    "    return f1_score\n",
    "\n",
    "\n",
    "def extract_cmi(graphical_array, values_array):\n",
    "    # List to store conditional mutual information (CMI) values\n",
    "    cmi_values = []\n",
    "    \n",
    "    # Define valid causal links in the graphical array\n",
    "    valid_arrows = {'-->', '<--', 'o-o', '-?>', '<?-', '<->', 'o->', '<-o'}\n",
    "    \n",
    "    # Iterate through both arrays to extract CMI values where there are causal arrows\n",
    "    for i in range(graphical_array.shape[0]):\n",
    "        for j in range(graphical_array.shape[1]):\n",
    "            ground_relation = graphical_array[i][j]\n",
    "            value_pair = values_array[i][j]\n",
    "            \n",
    "            # Check if the relationship is a valid causal arrow\n",
    "            if ground_relation in valid_arrows:\n",
    "                for value in value_pair:\n",
    "                    # Only consider finite values (exclude inf and -inf)\n",
    "                    if np.isfinite(value):\n",
    "                        cmi_values.append(value)\n",
    "    \n",
    "    # Compute the average of the collected CMI values\n",
    "    if len(cmi_values) > 0:\n",
    "        avg_cmi = np.mean(cmi_values)\n",
    "    else:\n",
    "        avg_cmi = 0  # No valid values to average\n",
    "    \n",
    "    return avg_cmi\n",
    "\n",
    "\n",
    "def latent_link_recall(ground_truth, predicted):\n",
    "    # Define valid latent causal links\n",
    "    latent_links = {'o-o', '<-o', 'o->', '<->'}\n",
    "    \n",
    "    true_positives = 0\n",
    "    false_negatives = 0\n",
    "    \n",
    "    # Iterate through both arrays to compare latent links\n",
    "    for i in range(ground_truth.shape[0]):\n",
    "        for j in range(ground_truth.shape[1]):\n",
    "            ground_relation = ground_truth[i][j]\n",
    "            predicted_relation = predicted[i][j]\n",
    "            \n",
    "            # Check for latent links in the ground truth\n",
    "            if ground_relation in latent_links:\n",
    "                if predicted_relation in latent_links:\n",
    "                    true_positives += 1  # Correctly identified latent link\n",
    "                else:\n",
    "                    false_negatives += 1  # Latent link missed by the prediction\n",
    "    \n",
    "    # Calculate recall\n",
    "    recall = true_positives / (true_positives + false_negatives) if true_positives + false_negatives > 0 else 0\n",
    "    \n",
    "    return recall\n",
    "\n",
    "def feature_selection_lpcmci(data, gt = None, lag=1, test_size=0.2, random_state=42, alpha=0.05, n_shuffles=100):\n",
    "\n",
    "        def create_lagged_features(df, column, lags):\n",
    "            lagged_df = pd.DataFrame()\n",
    "            lagged_df[column] = df[column]  # Include the original unlagged version\n",
    "            for lag in range(1, lags + 1):\n",
    "                lagged_df[f'{column}_lag{lag}'] = df[column].shift(lag)\n",
    "            lagged_df.dropna()\n",
    "            return lagged_df\n",
    "\n",
    "\n",
    "        def shuffle_column(df, column):\n",
    "            shuffled_df = df.copy()\n",
    "            shuffled_df = shuffled_df[[column]]\n",
    "            shuffled_df[column] = np.random.permutation(df[column].values)\n",
    "            return shuffled_df\n",
    "\n",
    "\n",
    "        def evaluate_significance(m_real, m_shuffles, alpha):\n",
    "            mu, sigma = norm.fit(m_shuffles)\n",
    "            p_value = norm.cdf(m_real, mu, sigma)\n",
    "            return p_value < alpha\n",
    "\n",
    "\n",
    "        def calculate_sma(series, window):\n",
    "            return series.rolling(window=window, min_periods=1).mean()\n",
    "\n",
    "\n",
    "        # Initialize the dictionary\n",
    "        num_vars = data.shape[1]\n",
    "        link_assumptions = {j: {(i, -tau): '' for i in range(num_vars) for tau in range(2) if (i, -tau) != (j, 0)} for j in range(num_vars)}\n",
    "\n",
    "        start = time.time()\n",
    "        for target in data.columns:\n",
    "            potential_drivers = [col for col in data.columns]\n",
    "            window = round(len(data) * .2)\n",
    "\n",
    "\n",
    "            sma_baseline = calculate_sma(data[target], window).dropna()\n",
    "            aligned_data = data.iloc[window-1:].reset_index(drop=True)\n",
    "\n",
    "\n",
    "            sma_baseline = sma_baseline.reset_index(drop=True)\n",
    "            y_baseline = aligned_data[target].reset_index(drop=True)\n",
    "\n",
    "\n",
    "            not_nan_index = y_baseline.dropna().index\n",
    "            sma_baseline = sma_baseline.loc[not_nan_index]\n",
    "            y_baseline = y_baseline.dropna()\n",
    "\n",
    "\n",
    "            X_baseline = sma_baseline.to_frame()\n",
    "\n",
    "\n",
    "            X_train, X_test, y_train, y_test = train_test_split(X_baseline, y_baseline, test_size=test_size, random_state=random_state)\n",
    "            baseline_model = RandomForestRegressor(random_state=random_state)\n",
    "            baseline_model.fit(X_train, y_train)\n",
    "            baseline_predictions = baseline_model.predict(X_test)\n",
    "            current_baseline_mse = mean_squared_error(y_test, baseline_predictions)\n",
    "\n",
    "\n",
    "            definite_drivers = []\n",
    "            discovered_drivers = []\n",
    "            definite_non_drivers = []\n",
    "            aligned_data = data.dropna()\n",
    "            combined_features_ = pd.DataFrame()\n",
    "\n",
    "\n",
    "            while potential_drivers:\n",
    "                current_driver = potential_drivers.pop(0)\n",
    "                driver_lagged = create_lagged_features(aligned_data, current_driver, lag).dropna()\n",
    "                best_mse = current_baseline_mse\n",
    "                best_lag = None\n",
    "            \n",
    "                for lag_num in tqdm(range(0, lag + 1), desc=f'Evaluating {current_driver} lags'):\n",
    "                    if lag_num == 0 and current_driver == target:\n",
    "                        continue\n",
    "                    elif lag_num == 0:\n",
    "                        current_driver_lagged = driver_lagged[[current_driver]]\n",
    "                    else:\n",
    "                        current_driver_lagged = driver_lagged[[f'{current_driver}_lag{lag_num}']]\n",
    "                    \n",
    "                    combined_features = pd.concat([combined_features_, current_driver_lagged], axis=1)\n",
    "                    target_column = aligned_data[target].loc[combined_features.index]\n",
    "                    X_train, X_test, y_train, y_test = train_test_split(combined_features, target_column, test_size=test_size, random_state=random_state)\n",
    "                    combined_model = RandomForestRegressor(random_state=random_state)\n",
    "                    combined_model.fit(X_train, y_train)\n",
    "                \n",
    "                    combined_predictions = combined_model.predict(X_test)\n",
    "                    combined_mse = mean_squared_error(y_test, combined_predictions)\n",
    "                    if combined_mse < best_mse:\n",
    "                        best_mse = combined_mse\n",
    "                        best_lag = lag_num\n",
    "                \n",
    "                if best_lag is not None:\n",
    "                    shuffle_mse = []\n",
    "                    for _ in range(n_shuffles):\n",
    "                        if best_lag == 0:\n",
    "                            shuffled_data = shuffle_column(driver_lagged, current_driver)\n",
    "                        else:\n",
    "                            shuffled_data = shuffle_column(driver_lagged, f'{current_driver}_lag{best_lag}')\n",
    "                        shuffled_features = pd.concat([combined_features_, shuffled_data], axis=1).dropna()\n",
    "                        X_train_shuffled, X_test_shuffled, y_train_shuffled, y_test_shuffled = train_test_split(shuffled_features, target_column, test_size=test_size, random_state=random_state)\n",
    "                        shuffled_model = RandomForestRegressor(random_state=random_state)\n",
    "                        shuffled_model.fit(X_train_shuffled, y_train_shuffled)\n",
    "                    \n",
    "                        shuffled_predictions = shuffled_model.predict(X_test_shuffled)\n",
    "                        shuffle_mse.append(mean_squared_error(y_test_shuffled, shuffled_predictions))\n",
    "                \n",
    "                    if evaluate_significance(best_mse, shuffle_mse, alpha):\n",
    "                        if best_lag == 0:\n",
    "                            definite_drivers.append(current_driver)\n",
    "                        else:\n",
    "                            definite_drivers.append(f'{current_driver}_lag{best_lag}')\n",
    "                        current_baseline_mse = best_mse\n",
    "                        combined_features_ = pd.concat([combined_features_, current_driver_lagged], axis=1)\n",
    "                    else:\n",
    "                        discovered_drivers.append(current_driver if best_lag == 0 else f'{current_driver}_lag{best_lag}')\n",
    "                else:\n",
    "                    definite_non_drivers.append(current_driver)\n",
    "\n",
    "            print(f\"Target: {target}\")\n",
    "            print(f\"Definite Drivers: {definite_drivers}\")\n",
    "            print(f\"Discovered Drivers: {discovered_drivers}\")\n",
    "            print(f\"Definite Non-Drivers: {definite_non_drivers}\")\n",
    "            print(f\"Final Baseline MSE: {current_baseline_mse}\")\n",
    "\n",
    "            # Update link_assumptions dictionary\n",
    "            target_idx = data.columns.get_loc(target)\n",
    "            for driver in definite_drivers:\n",
    "                if '_lag' in driver:\n",
    "                    driver_name, lag_value = driver.rsplit('_lag', 1)\n",
    "                    driver_idx = data.columns.get_loc(driver_name)\n",
    "                    lag_value = -int(lag_value)\n",
    "                    link_assumptions[target_idx][(driver_idx, lag_value)] = '-->'\n",
    "                else:\n",
    "                    driver_idx = data.columns.get_loc(driver)\n",
    "                    link_assumptions[target_idx][(driver_idx, 0)] = '-->'\n",
    "                    link_assumptions[driver_idx][(target_idx, 0)] = '<--'\n",
    "\n",
    "            for driver in discovered_drivers:\n",
    "                if '_lag' in driver:\n",
    "                    driver_name, lag_value = driver.rsplit('_lag', 1)\n",
    "                    driver_idx = data.columns.get_loc(driver_name)\n",
    "                    lag_value = -int(lag_value)\n",
    "                    link_assumptions[target_idx][(driver_idx, lag_value)] = '-?>'\n",
    "                else:\n",
    "                    driver_idx = data.columns.get_loc(driver)\n",
    "                    link_assumptions[target_idx][(driver_idx, 0)] = '-?>'\n",
    "                    link_assumptions[driver_idx][(target_idx, 0)] = '<?-'\n",
    "\n",
    "        for target in data.columns:\n",
    "            target_idx = data.columns.get_loc(target)\n",
    "            for driver in discovered_drivers + definite_drivers:\n",
    "                if '_lag' in driver:\n",
    "                    driver_name, lag_value = driver.rsplit('_lag', 1)\n",
    "                    lag_value = -int(lag_value)\n",
    "                    driver_idx = data.columns.get_loc(driver_name)\n",
    "                    if target_idx != driver_idx: \n",
    "                        if link_assumptions[driver_idx][(target_idx, lag_value)] in ['-?>', '-->'] and link_assumptions[target_idx][(driver_idx, lag_value)] in ['<?-', '<--']:\n",
    "                            link_assumptions[target_idx][(driver_idx, 0)] = 'o?o'\n",
    "                            link_assumptions[driver_idx][(target_idx, 0)] = 'o?o'\n",
    "                else: \n",
    "                    driver_idx = data.columns.get_loc(driver)\n",
    "                    if target_idx != driver_idx: \n",
    "                        if link_assumptions[driver_idx][(target_idx, 0)] in ['-?>', '-->'] and link_assumptions[target_idx][(driver_idx, 0)] in ['<?-', '<--']:\n",
    "                            link_assumptions[target_idx][(driver_idx, 0)] = 'o?o'\n",
    "                            link_assumptions[driver_idx][(target_idx, 0)] = 'o?o'\n",
    "        print(link_assumptions)\n",
    "        n_a_n = np.isnan(data).any(axis=1)\n",
    "        data[n_a_n] = 999\n",
    "        data = data.values\n",
    "        data = pp.DataFrame(data, var_names = [\"y0\", \"y1\", \"y2\", \"y3\", \"y4\"], missing_flag = 999)\n",
    "        cmi_knn = CMIknn(significance='shuffle_test', knn=0.1, shuffle_neighbors=5, transform='ranks', sig_samples=200)\n",
    "        print(\"Starting LPCMCI...\")\n",
    "        lpcmci_loc = LPCMCI(\n",
    "            dataframe=data, \n",
    "            cond_ind_test=cmi_knn,\n",
    "            verbosity=0)\n",
    "        results = lpcmci_loc.run_lpcmci(tau_max=1, pc_alpha=.2, link_assumptions = link_assumptions, n_preliminary_iterations = 0)\n",
    "        end = time.time()\n",
    "        #tp.plot_time_series_graph(graph=results['graph'],\n",
    "        #                        val_matrix=results['val_matrix'])\n",
    "        if gt == None:\n",
    "            f1 = 0\n",
    "        else:\n",
    "            f1 = compute_f1_score(gt, results['graph'])\n",
    "\n",
    "        cmi_val = extract_cmi(results['graph'], results['val_matrix'])\n",
    "        recall_latent = latent_link_recall(gt, results['graph'])\n",
    "        elapsed_time = end - start\n",
    "        print(\"Finished.\")\n",
    "        return results, elapsed_time, f1, cmi_val, recall_latent\n",
    "dat = pd.read_csv('/home/gnicolaou/tigramite/tutorials/causal_discovery/data.csv')\n",
    "result = feature_selection_lpcmci(dat)\n",
    "print(result)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
